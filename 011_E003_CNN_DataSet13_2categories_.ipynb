{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoFyIizw-YaT"
   },
   "source": [
    "CNN LSTM \n",
    "\n",
    "Data Set 14\n",
    "Using data zero data zero padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISksYe7Fr3LI"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM, Conv1D\n",
    "from keras.layers import Lambda, Convolution1D, MaxPooling1D, Flatten, Reshape, BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.metrics import categorical_crossentropy, binary_crossentropy\n",
    "\n",
    "#from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "#For data saving\n",
    "import pickle\n",
    "import random\n",
    "#other imports\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "import keras.backend as K\n",
    "import os\n",
    "import time \n",
    "#cwd = os.path.dirname(os.path.realpath(\"SURF_001_TwoClass13.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVqz_9kQi7bd"
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5AyunGGYTSd"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbM8m6yq2ZIX"
   },
   "source": [
    "**Creating Dataframe**\n",
    "\n",
    "https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(1000, 4), n_channels=0,\n",
    "                 n_classes=2, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, 1000, 4))\n",
    "        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('../model_11_data_4/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-06FGqcExs6j"
   },
   "outputs": [],
   "source": [
    "dense_layer = 1\n",
    "layer_size = 64 \n",
    "conv_layer = 2\n",
    "seq_length = 1000\n",
    "base_pair = 4\n",
    "num_strides = 1\n",
    "pool_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = 0\n",
    "with open('../partition_train_val_a.dat', 'rb') as f:\n",
    "    partition = pickle.load(f)\n",
    "for index in partition['train']:\n",
    "    a = partition['labels'][index][0][0]\n",
    "    if test_count < 10:\n",
    "        print(a)\n",
    "    partition['labels'][index] = a\n",
    "    test_count += 1\n",
    "    if test_count < 10:\n",
    "        print(partition['labels'][index])\n",
    "for index in partition['validation']:\n",
    "    a = partition['labels'][index][0][0]\n",
    "    partition['labels'][index] = a\n",
    "    test_count += 1\n",
    "    if test_count < 20:\n",
    "        print(partition['labels'][index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1 = 0\n",
    "cat_0 = 0\n",
    "for i in partition['train']:\n",
    "    if partition['labels'][i] == 0:\n",
    "        cat_0 += 1\n",
    "    elif partition['labels'][i] == 1:\n",
    "        cat_1 += 1\n",
    "    else:\n",
    "        print('issue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_1)\n",
    "print(cat_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (32,1000,4),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "#with open('../partition_train_val_a.dat', 'rb') as f:\n",
    "   # partition = pickle.load(f)\n",
    "\n",
    "labels = partition['labels']\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "\n",
    "# Design model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=layer_size, kernel_size=(4), input_shape=(seq_length, 4), activation='relu', strides=1, padding='valid'))\n",
    "model.add(Conv1D(layer_size, 4, batch_input_shape=(32, seq_length, 4), strides=1, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(12))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = partition['train'][0]\n",
    "print(a)\n",
    "print(partition['labels'][a] )\n",
    "train_labels = np.empty(1, dtype=np.int8)\n",
    "train_samples = []\n",
    "train_labels[0] = partition['labels'][a] \n",
    "train_samples = np.load('../model_11_data_4/' + partition['train'][0] + '.npy')\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(train_samples.shape)\n",
    "\n",
    "val_labels = train_labels\n",
    "val_samples = train_samples\n",
    "print(val_labels.shape)\n",
    "print(val_samples.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (32,1000,4),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 2,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "#with open('../partition_train_val_a.dat', 'rb') as f:\n",
    "   # partition = pickle.load(f)\n",
    "\n",
    "labels = partition['labels']\n",
    "\n",
    "# Generators\n",
    "#training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "#validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "\n",
    "dense_layers = [0, 1]\n",
    "layer_sizes = [32, 64]\n",
    "conv_layers = [0,1]\n",
    "\n",
    "# Design model\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            Earlystop = EarlyStopping(\n",
    "            monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n",
    "            baseline=None, restore_best_weights=True)\n",
    "            count_input_train = 0\n",
    "            count_input_validation = 0\n",
    "            while count_input <= len(partition['train']):\n",
    "                Y_train = np.zeros((200, 1), dtype=np.int8)\n",
    "                X_train = np.zeros((200, 1000, 4), dtype=np.int8)\n",
    "                Y_val = np.zeros((40, 1), dtype=np.int8)\n",
    "                X_val = np.zeros((40, 1000, 4), dtype=np.int8)\n",
    "                for i in range(0, 200):\n",
    "                    a = partition['train'][count_input]\n",
    "                    X_train[i, :, :] = np.load('../model_11_data_4/' + partition['train'][count_input_train] + '.npy') \n",
    "                    Y_train[i, :] = partition['labels'][a] \n",
    "                    count_input_train += 1\n",
    "                for i in range(0, 40):\n",
    "                    a = partition['validation'][count_input]\n",
    "                    X_val[i, :, :] = np.load('../model_11_data_4/' + partition['validation'][count_input_validation] + '.npy') \n",
    "                    Y_val[i, :] = partition['labels'][a] \n",
    "                    count_input_validation += 1\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Conv1D(filters=layer_size, kernel_size=(4), input_shape=(seq_length, 4), activation='relu', strides=1, padding='valid'))\n",
    "                model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "                model.add(Conv1D(layer_size, 4, batch_input_shape=(1, seq_length, 4), strides=1, padding='valid', activation='relu'))\n",
    "                model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "\n",
    "                model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "                for l in range(conv_layer-1):\n",
    "                    model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "                    model.add(Conv1D(layer_size, 4, activation='relu'))\n",
    "\n",
    "    #model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last'))\n",
    "    #model.add(Conv1D(64, 4, activation='relu'))\n",
    "\n",
    "                model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "                model.add(LSTM(32))\n",
    "\n",
    "                model.add(Flatten())\n",
    "\n",
    "                for l in range(dense_layer):\n",
    "                    model.add(Dense(12))\n",
    "                    model.add(Activation('relu'))\n",
    "                model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "                model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "                if(count_input % 10000 == 0):\n",
    "                    print(model.summary())\n",
    "                    model.save('../Models_H5/011_{}-conv-{}-nodes-{}-dense-{}CNNLSTM'.format(conv_layer, layer_size, dense_layer, int(time.time())))\n",
    "                # Train model on dataset\n",
    "                model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, callbacks=[Earlystop])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbM8m6yq2ZIX"
   },
   "source": [
    "**OLD CODE BELOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbM8m6yq2ZIX"
   },
   "source": [
    "**Preparing inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "wMd_bq0XYSkx",
    "outputId": "0bc90043-ae4c-431a-ad40-9a767bf5118f"
   },
   "outputs": [],
   "source": [
    "with open('Version_e/' + str(int(0 / 20000))+'/' + str(int(0 / 100))+'/DataSet_14_train_labels_onehot_batches1_Part_' + str(1) + '.dat', 'rb') as f:\n",
    "  train_labels = pickle.load(f)\n",
    "with open('Version_f/' + str(int(0 / 20000))+'/' + str(int(0 / 100))+'/DataSet_14_train_samples_onehot_batches1_Part_' + str(1) + '.dat', 'rb') as f:\n",
    "  train_samples = pickle.load(f)\n",
    "print(train_labels.shape)\n",
    "print(train_samples.shape)\n",
    "\n",
    "with open('Version_e/' + str(int(0 / 20000))+'/' + str(int(0 / 100))+'/DataSet_14_train_labels_onehot_batches1_Part_' + str(1) + '.dat', 'rb') as f:\n",
    "  test_labels = pickle.load(f)\n",
    "with open('Version_f/' + str(int(0 / 20000))+'/' + str(int(0 / 100))+'/DataSet_14_train_samples_onehot_batches1_Part_' + str(1) + '.dat', 'rb') as f:\n",
    "  test_samples = pickle.load(f)\n",
    "print(test_labels.shape)\n",
    "print(test_samples.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_labels_onehot_Part:' + str(1) + '.dat', 'rb') as f:\n",
    "  train_labels = pickle.load(f)\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_samples_onehot_Part:' + str(1) + '.dat', 'rb') as f:\n",
    "  train_samples = pickle.load(f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3swzw6VREfGe",
    "outputId": "c61f2642-b49f-41ff-f9bc-ef5b17278c75"
   },
   "outputs": [],
   "source": [
    "print(train_labels)\n",
    "print(train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCuEJ5px4GLB"
   },
   "source": [
    "CNN LSTM Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWgSy-mqZLwY"
   },
   "source": [
    "Notes:\n",
    "\n",
    "Seems to crash when data is stored in batches of 200 and largerst seq length 310400\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WYfdAj_ddQR-",
    "outputId": "8c16abd4-38cd-4033-8152-21db6849fd67"
   },
   "outputs": [],
   "source": [
    "\n",
    "# input  images of 4 by 200\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=layer_size, kernel_size=(4), input_shape=(seq_length, 4), activation='relu', strides=1, padding='valid'))\n",
    "model.add(Conv1D(layer_size, 4, batch_input_shape=(1, seq_length, 4), strides=1, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "\n",
    "model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "for l in range(conv_layer-1):\n",
    "  model.add(MaxPooling1D(pool_size=4, strides=None, padding='valid', data_format='channels_last'))\n",
    "  model.add(Conv1D(layer_size, 4, activation='relu'))\n",
    "\n",
    "#model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last'))\n",
    "#model.add(Conv1D(64, 4, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "model.add(LSTM(32))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "for l in range(dense_layer):\n",
    "  model.add(Dense(12))\n",
    "  model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(train_samples, train_labels, validation_data=(test_samples, test_labels), epochs=10)\n",
    "\n",
    "'''\n",
    "odel = Sequential()\n",
    "model.add(Conv1D(filters=layer_size, kernel_size=(4), input_shape=(seq_length, 4), activation='relu', strides=1, padding='valid'))\n",
    "model.add(MaxPooling1D(pool_size=4, batch_input_shape=(1, seq_length, 4), strides=1, padding='valid', data_format='channels_last'))\n",
    "model.add(Conv1D(layer_size, 4, strides=1, padding='valid', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4, strides=1, padding='valid', data_format='channels_last'))\n",
    "model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "for l in range(conv_layer-1):\n",
    "  model.add(Conv1D(layer_size, 4, strides=1, padding='valid', activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=4, strides=1, padding='valid', data_format='channels_last'))\n",
    "  model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "#model.add(LSTM(64))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#for l in range(dense_layer):\n",
    "#  model.add(Dense(64))\n",
    "#  model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#callbacks=[]\n",
    "model.fit(train_samples, train_labels, validation_data=(test_samples, test_labels), epochs=3)\n",
    "model.save('gdrive/My Drive/SURF_2020_Weiss/Models_H5/008_DataSet13_{}-conv-{}-nodes-{}-dense-200-epochs-8-version-Part:1-CNNLSTM'.format(conv_layer, layer_size, dense_layer))\n",
    "#version 3 is with all the chromosomes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LJqamDnuMV7y",
    "outputId": "68f3496d-60db-48ef-faee-5a8e6be0cf7f"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "num_place = 2\n",
    "\n",
    "#1 - 2411\n",
    "for part in range(3, 54300):\n",
    "  num_place += 1\n",
    "  if num_place > 20000:\n",
    "    num_place -= 20000\n",
    "  if part != 1:\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/' + str(int(part / 20000))+'/' + str(int(num_place / 100))+'/DataSet_13_train_labels_onehot_batches5_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_labels = pickle.load(f)\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/' + str(int(part / 20000))+'/' + str(int(num_place / 100))+'/DataSet_13_train_samples_onehot_batches5_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_samples = pickle.load(f)\n",
    "\n",
    "    shuffle = np.random.permutation(len(train_samples))\n",
    "    train_samples = train_samples[shuffle]\n",
    "    train_labels = train_labels[shuffle]\n",
    "    \n",
    "    model.fit(train_samples, train_labels, validation_data=(test_samples, test_labels), epochs=3)\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    if part % 100 == 0 or part == 50:\n",
    "      print(model.summary())\n",
    "      print('Part:' + str(part))\n",
    "      model.save('gdrive/My Drive/SURF_2020_Weiss/008_Model/Models_H5/008_DataSet13_{}-conv-{}-nodes-{}-dense-zeroPadding-3-epochs-2-version-Part:{}-CNNLSTM'.format(conv_layer, layer_size, dense_layer, part))\n",
    "# Versions for Model 008\n",
    "# Version 1: 1 bat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcxLa9pLEmc4"
   },
   "source": [
    "**Versions:**\n",
    "\n",
    "Version 5:\n",
    "  - Issue not loading data\n",
    "  - Data set 13, data: onehot version with batches of 1, model: batches of 1\n",
    "\n",
    "Version 6:\n",
    " - Got ride of loading, saving, and compiling model each time, ran with data set 13, data: onehot version with batches of 1, model: batches of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ch2BMY1Hl4vk"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs\n",
    "#%tensorboard --logdir logs/fit\n",
    "#%tensorboard --logdir gdrive/My Drive/SU\n",
    "#RF_2020_Weiss/Models/E003_CNN_2category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "puBSw8XWyVPG",
    "outputId": "b46749ca-d3f7-4552-e79b-ac49e276083d"
   },
   "outputs": [],
   "source": [
    "model.save('gdrive/My Drive/SURF_2020_Weiss/008_Model/Models_H5/008_DataSet13_{}-conv-{}-nodes-{}-dense-zeroPadding-3-epochs-2-version-Part:{}-CNNLSTM'.format(conv_layer, layer_size, dense_layer, part))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrsZ_99P8Bcm"
   },
   "source": [
    "Testing the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "W_vw1pFO8FP0",
    "outputId": "f765b4b0-f705-4079-e7f4-aedea24b11b1"
   },
   "outputs": [],
   "source": [
    "# Loading testing data: \n",
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_samples_onehot_200bp' + genome[13] + '.dat', 'rb') as f:\n",
    "  test_samples = pickle.load(f)\n",
    "  print(test_samples.shape)\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_labels_onehot_200bp' + genome[13] + '.dat', 'rb') as f:\n",
    "  test_labels = pickle.load(f)\n",
    "  print(test_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIG_bTgY8FVa"
   },
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = load_model('gdrive/My Drive/SURF_2020_Weiss/Models_H5/006_DataSet10_2-conv-64-nodes-0-dense-200-epochs-3-version-chrX-CNNLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "py2G59HW8wwx",
    "outputId": "36335c24-8568-4a6f-9f70-8a1bc89beeea"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict_classes(test_samples)\n",
    "print(len(predicted))\n",
    "print(predicted[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rQjbrLcN8FZc",
    "outputId": "5621de35-98c1-42f3-9043-82faac6aff06"
   },
   "outputs": [],
   "source": [
    "expected = []\n",
    "for label in test_labels:\n",
    "  for i, num in enumerate(label):\n",
    "    if num == 1:\n",
    "      expected.append(i)\n",
    "count = 0 \n",
    "count_2 = 0\n",
    "for x in predicted:\n",
    "  if x != 0:\n",
    "    count += 1\n",
    "for y in expected:\n",
    "  if y != 0:\n",
    "    count_2 += 1\n",
    "print(count)\n",
    "print(count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aTt3ectT0vUp",
    "outputId": "4d19299e-ac34-4e51-9467-23fa453f5f69"
   },
   "outputs": [],
   "source": [
    "print(expected[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_jj88sSuXE4_",
    "outputId": "026e5697-fe97-44c4-d0c1-4ab02156a496"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i, num in enumerate(expected):\n",
    "  if num == 1 and predicted[i] == 1: \n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "colab_type": "code",
    "id": "UtXDeRUf8FcJ",
    "outputId": "1dac6d6b-578d-4e76-a902-ee12920ce29c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "results_2 = multilabel_confusion_matrix(expected, predicted)\n",
    "print(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4k-EXHNp8Fhy"
   },
   "outputs": [],
   "source": [
    "testing = [1, 2, 3, 3]\n",
    "testing_2 = [1, 2, 3 , 3]\n",
    "results_2 = multilabel_confusion_matrix(testing, testing_2)\n",
    "print(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFLDeml38Fff"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwAOFewy7-2X"
   },
   "source": [
    "Extra Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLBMGfufTtBE"
   },
   "outputs": [],
   "source": [
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_samples_onehot_200bp' + genome[7] + '.dat', 'rb') as f:\n",
    "  test_samples = pickle.load(f)\n",
    "  print(test_samples.shape)\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_labels_onehot_200bp' + genome[7] + '.dat', 'rb') as f:\n",
    "  test_labels = pickle.load(f)\n",
    "  print(test_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPfOnM6TT1yl"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict_classes(test_labels)\n",
    "print(len(predicted))\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GopWj3CuUOoC"
   },
   "outputs": [],
   "source": [
    "expected = []\n",
    "for label in test_labels:\n",
    "  for i, num in enumerate(label):\n",
    "    if num == 1:\n",
    "      expected.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mg1W5Ee5V69l",
    "outputId": "c3996658-29d1-4d05-998d-0fa9b237d3c2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example of a confusion matrix in Python\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)\n",
    "# Printing the precision and recall, among other metrics\n",
    "#print(metrics.classification_report(y_act, y_pred, labels=[\"a\", \n",
    "#\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gdeXDpgZlcNv",
    "outputId": "83cc0e28-b070-4e74-b895-f3a72685949c"
   },
   "outputs": [],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "-4LNjfXQle4Z",
    "outputId": "e59d991f-0a12-4a49-8849-b72e6e359f62"
   },
   "outputs": [],
   "source": [
    "train_labels.shape\n",
    "print(train_labels[0][0])\n",
    "train_labels_flat = []\n",
    "for i in range(0, len(train_labels)):\n",
    "  train_labels_flat.append(train_labels[i][0])\n",
    "print(len(train_labels))\n",
    "\n",
    "test_labels.shape\n",
    "print(test_labels[0][0])\n",
    "test_labels_flat = []\n",
    "for i in range(0, len(test_labels)):\n",
    "  test_labels_flat.append(test_labels[i][0])\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LX5ieiuzmOz4",
    "outputId": "326464f2-3834-422e-a70b-ce381749025f"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in test_labels:\n",
    "  if i[0] == 1:\n",
    "    count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "udF9Q9JBYKqg",
    "outputId": "17edd373-7c6a-45f1-9547-2312f6daa273"
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_samples, test_samples, train_labels_flat, test_labels_flat\n",
    "\n",
    "nsamples, nx, ny = X_train.shape\n",
    "X_train = X_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "#nsamples, nx, ny = y_train.shape\n",
    "#y_train = y_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = X_test.shape\n",
    "X_test = X_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "#nsamples, nx, ny = y_test.shape\n",
    "#y_test = y_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YM_JkE12kQbU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "011_E003_CNN_DataSet13_2categories_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
