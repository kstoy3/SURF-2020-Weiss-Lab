{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LQ3AGvRiS6y"
   },
   "source": [
    "Binary heterochromatin classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwWEdAva_945"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM\n",
    "from keras.layers import Lambda, Convolution1D, MaxPooling1D, Flatten, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import categorical_crossentropy, binary_crossentropy\n",
    "#For data saving\n",
    "import pickle\n",
    "import random\n",
    "#other imports\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "import keras.backend as K\n",
    "import os\n",
    "#cwd = os.path.dirname(os.path.realpath(\"SURF_001_TwoClass13.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMMmIQk3Rvc"
   },
   "source": [
    "Loading algorithims and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59vW0oPKF4NF"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-qiIbo-AZgG"
   },
   "outputs": [],
   "source": [
    "def oneHot_DNA(chrom):\n",
    "    one_hot_full = np.zeros((len(chrom), len(chrom[0]), 4), dtype=np.int8)\n",
    "    for i, seq in enumerate(chrom):\n",
    "        seq_onehot = np.zeros((len(seq), 4))\n",
    "        for j, nuc in enumerate(seq):\n",
    "            if nuc == 'a':\n",
    "                seq_onehot[j, :] = np.array([1, 0, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 't':\n",
    "                seq_onehot[j, :] = np.array([0, 1, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 'c':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 1, 0], dtype=np.int8)\n",
    "            elif nuc == 'g':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 0, 1], dtype=np.int8)\n",
    "            one_hot_full[i,:,:] = seq_onehot\n",
    "    return one_hot_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NBzys0oburV"
   },
   "outputs": [],
   "source": [
    "# Do 2D onhot arrays and feed each full sequence into the nn one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fREkD6bmc_cN"
   },
   "outputs": [],
   "source": [
    "def oneHot_DNA_full_PaddingZeros(chrom, max):\n",
    "    largest_size = max\n",
    "    #for seq in chrom:\n",
    "    #  size = len(seq)\n",
    "    #  if size > largest_size:\n",
    "    #    largest_size = size\n",
    "    one_hot_full = np.zeros((len(chrom), largest_size, 4), dtype=np.int8)\n",
    "    for i, seq in enumerate(chrom):\n",
    "        seq_onehot = np.zeros((largest_size, 4))\n",
    "        for j, nuc in enumerate(seq):\n",
    "            if nuc == 'a':\n",
    "                seq_onehot[j, :] = np.array([1, 0, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 't':\n",
    "                seq_onehot[j, :] = np.array([0, 1, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 'c':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 1, 0], dtype=np.int8)\n",
    "            elif nuc == 'g':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 0, 1], dtype=np.int8)\n",
    "            else:\n",
    "                print('issue')\n",
    "            one_hot_full[i,:,:] = seq_onehot\n",
    "    return one_hot_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYSMMQOvVwiU"
   },
   "outputs": [],
   "source": [
    "def oneHot_DNA_full_PaddingZeros_1batch(chrom, max):\n",
    "    largest_size = max\n",
    "  #for seq in chrom:\n",
    "  #  size = len(seq)\n",
    "  #  if size > largest_size:\n",
    "  #    largest_size = size\n",
    "    one_hot_full = np.zeros((1, largest_size, 4), dtype=np.int8)\n",
    "    for i, seq in enumerate(chrom):\n",
    "        seq_onehot = np.zeros((largest_size, 4))\n",
    "        for j, nuc in enumerate(seq):\n",
    "            if nuc == 'a':\n",
    "                seq_onehot[j, :] = np.array([1, 0, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 't':\n",
    "                seq_onehot[j, :] = np.array([0, 1, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 'c':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 1, 0], dtype=np.int8)\n",
    "            elif nuc == 'g':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 0, 1], dtype=np.int8)\n",
    "            else:\n",
    "                print('issue')\n",
    "            one_hot_full[i,:,:] = seq_onehot\n",
    "    return one_hot_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2JQgp7YSz6S"
   },
   "outputs": [],
   "source": [
    "chrom = [['a', 'a', 'a'], ['t', 't'], ['c', 'c', 'c','c'], ['g','g','g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VA55H3idnfgG"
   },
   "outputs": [],
   "source": [
    "def oneHot_labels2_1batch(chrom):\n",
    "    seq_onehot = np.zeros((1, 2), dtype=np.int8)\n",
    "    if chrom == 1:\n",
    "        seq_onehot[0, :] = np.array([1, 0], dtype=np.int8)\n",
    "    elif chrom == 0:\n",
    "        seq_onehot[0, :] = np.array([0, 1], dtype=np.int8)\n",
    "    else:\n",
    "        print(\"issue - with the labels\")\n",
    "    return seq_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06qGFesc3gSD"
   },
   "source": [
    "Onehot encoding the samples and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "_XRVBWpeg2NY",
    "outputId": "5cbf8478-0c71-42f3-f9e8-c3a47a5178cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223800\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n",
      "223836\n"
     ]
    }
   ],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "\n",
    "max_num = 0\n",
    "for x, indexes in enumerate(genome):\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_test_samples_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        test_samples = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_samples = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_test_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        test_labels = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_labels = pickle.load(f2)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        if label == 0:\n",
    "            if max_num < len(train_samples[i]):\n",
    "                max_num = len(train_samples[i])\n",
    "    for i, label in enumerate(test_labels):\n",
    "        if label == 0:\n",
    "            if max_num < len(test_samples[i]):\n",
    "                 max_num = len(test_samples[i])\n",
    "    print(max_num)\n",
    "    print(max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2b4hp3QnfMPk"
   },
   "outputs": [],
   "source": [
    "largest_seq = max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2-icGZ0Krkkz",
    "outputId": "a30af7e1-8849-4b4e-a661-e24d55f69b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n",
      "(1, 1000, 4)\n",
      "0\n",
      "[[0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n            count_a += 1\\n            count_a_place += 1\\n            #  print(\"count_a\")\\n            with open(\\'../DataSet_14_Part_1/\\' + str(int(count_a / 10000))+\\'/\\' + str(int(count_a_place / 100))+\\'/DataSet_14_train_labels_onehot_batches1_Part_\\' + str(count_a) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_labels, f)\\n            with open(\\'../DataSet_14_Part_1/\\' + str(int(count_a / 10000))+\\'/\\' + str(int(count_a_place / 100))+\\'/DataSet_14_train_samples_onehot_batches1_Part_\\' + str(count_a) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_samples, f)\\n        elif len_seq > 1000 and len_seq <= 10000:\\n            new_train_samples = oneHot_DNA_full_PaddingZeros_1batch(index_array, 10000)\\n            new_train_labels = oneHot_labels2_1batch(train_labels[y])\\n            count_b += 1\\n            count_b_place += 1\\n            #  print(\\'count_b:\\' + str(count_b))\\n            #  print(\\'count_b_place:\\' + str(count_b_place))\\n            #  print(\"count_b\")\\n            with open(\\'../DataSet_14_Part_2/\\' + str(int(count_b / 10000))+\\'/\\' + str(int(count_b_place / 100))+\\'/DataSet_14_train_labels_onehot_batches1_Part_\\' + str(count_b) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_labels, f)\\n            with open(\\'../DataSet_14_Part_2/\\' + str(int(count_b / 10000))+\\'/\\' + str(int(count_b_place / 100))+\\'/DataSet_14_train_samples_onehot_batches1_Part_\\' + str(count_b) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_samples, f)\\n        elif len_seq > 10000:\\n            count_c += 1\\n            count_c_place += 1\\n            #print(\"count_c\")\\n            new_train_samples = oneHot_DNA_full_PaddingZeros_1batch(index_array, max_num)\\n            new_train_labels = oneHot_labels2_1batch(train_labels[y])\\n            with open(\\'../DataSet_14_Part_3/\\' + str(int(count_c / 10000))+\\'/\\' + str(int(count_c_place / 100))+\\'/DataSet_14_train_labels_onehot_batches1_Part_\\' + str(count_c) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_labels, f)\\n            with open(\\'../DataSet_14_Part_3/\\' + str(int(count_c / 10000))+\\'/\\' + str(int(count_c_place / 100))+\\'/DataSet_14_train_samples_onehot_batches1_Part_\\' + str(count_c) + \\'.dat\\', \\'wb\\') as f:\\n                pickle.dump(new_train_samples, f)\\n        else:\\n            print(error)\\n        if count % 100 == 0:\\n            print(\"Count:\" + str(count))\\n\\n        count += 1\\n    '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_a = 0\n",
    "count_a_place = 0\n",
    "count_b = 0\n",
    "count_b_place = 0\n",
    "count_c = 0\n",
    "count_c_place = 0\n",
    "count = 0\n",
    "\n",
    "for x, indexes in enumerate(genome):\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "        train_samples = pickle.load(f1)\n",
    "        #print(len(train_samples))\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_labels = pickle.load(f2)\n",
    "        #print(len(train_labels))\n",
    "    for y, index in enumerate(train_samples):\n",
    "        len_seq = len(index)\n",
    "        new_train_samples = []\n",
    "        new_train_labels = []\n",
    "        index_array = []\n",
    "        index_array.append(index)\n",
    "        if count_a_place > 10000:\n",
    "            count_a_place -= 10000\n",
    "        if count_b_place > 10000:\n",
    "            count_b_place -= 10000\n",
    "        if count_c_place > 10000:\n",
    "            count_c_place -= 10000\n",
    "        if len_seq <= 1000:\n",
    "            if count > 9990 and count < 10000:\n",
    "                new_train_samples = oneHot_DNA_full_PaddingZeros(index_array, 1000)\n",
    "                new_train_labels = oneHot_labels2_1batch(train_labels[y])\n",
    "            if count > 9990 and count < 10000:\n",
    "                print(new_train_samples.shape)\n",
    "                print(train_labels[y])\n",
    "                print(new_train_labels)\n",
    "            count += 1\n",
    "'''\n",
    "\n",
    "            count_a += 1\n",
    "            count_a_place += 1\n",
    "            #  print(\"count_a\")\n",
    "            with open('../DataSet_14_Part_1/' + str(int(count_a / 10000))+'/' + str(int(count_a_place / 100))+'/DataSet_14_train_labels_onehot_batches1_Part_' + str(count_a) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_labels, f)\n",
    "            with open('../DataSet_14_Part_1/' + str(int(count_a / 10000))+'/' + str(int(count_a_place / 100))+'/DataSet_14_train_samples_onehot_batches1_Part_' + str(count_a) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_samples, f)\n",
    "        elif len_seq > 1000 and len_seq <= 10000:\n",
    "            new_train_samples = oneHot_DNA_full_PaddingZeros_1batch(index_array, 10000)\n",
    "            new_train_labels = oneHot_labels2_1batch(train_labels[y])\n",
    "            count_b += 1\n",
    "            count_b_place += 1\n",
    "            #  print('count_b:' + str(count_b))\n",
    "            #  print('count_b_place:' + str(count_b_place))\n",
    "            #  print(\"count_b\")\n",
    "            with open('../DataSet_14_Part_2/' + str(int(count_b / 10000))+'/' + str(int(count_b_place / 100))+'/DataSet_14_train_labels_onehot_batches1_Part_' + str(count_b) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_labels, f)\n",
    "            with open('../DataSet_14_Part_2/' + str(int(count_b / 10000))+'/' + str(int(count_b_place / 100))+'/DataSet_14_train_samples_onehot_batches1_Part_' + str(count_b) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_samples, f)\n",
    "        elif len_seq > 10000:\n",
    "            count_c += 1\n",
    "            count_c_place += 1\n",
    "            #print(\"count_c\")\n",
    "            new_train_samples = oneHot_DNA_full_PaddingZeros_1batch(index_array, max_num)\n",
    "            new_train_labels = oneHot_labels2_1batch(train_labels[y])\n",
    "            with open('../DataSet_14_Part_3/' + str(int(count_c / 10000))+'/' + str(int(count_c_place / 100))+'/DataSet_14_train_labels_onehot_batches1_Part_' + str(count_c) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_labels, f)\n",
    "            with open('../DataSet_14_Part_3/' + str(int(count_c / 10000))+'/' + str(int(count_c_place / 100))+'/DataSet_14_train_samples_onehot_batches1_Part_' + str(count_c) + '.dat', 'wb') as f:\n",
    "                pickle.dump(new_train_samples, f)\n",
    "        else:\n",
    "            print(error)\n",
    "        if count % 100 == 0:\n",
    "            print(\"Count:\" + str(count))\n",
    "\n",
    "        count += 1\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI3WU5r3b9gJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "part = 0 \n",
    "\n",
    "for x, indexes in enumerate(genome):\n",
    "  test_samples = []\n",
    "  test_labels = []\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_test_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "    test_samples = pickle.load(f1)\n",
    "      #print(len(train_samples))\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_test_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "    test_labels = pickle.load(f2)\n",
    "    print(len(test_samples))\n",
    "    print(len(test_labels))\n",
    "  \n",
    "  test_samples = oneHot_DNA_full_PaddingZeros(test_samples, largest_seq)\n",
    "  print(test_samples.shape)\n",
    "\n",
    "  test_labels = oneHot_labels2(test_labels)\n",
    "  print(test_labels.shape)\n",
    "\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/testing_data/DataSet_13_test_labels_onehot_batches5_Chr:{}.dat'.format(indexes), 'wb') as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "    print(test_labels.shape)\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/testing_data/DataSet_13_test_samples_onehot_batches5_Chr:{}.dat'.format(indexes), 'wb') as f:\n",
    "    pickle.dump(test_samples, f)\n",
    "    print(test_samples.shape)\n",
    "'''\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWJOodTHjaHC"
   },
   "outputs": [],
   "source": [
    "with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/testing_data/DataSet_13_test_labels_onehot_batches5_Chr:chr1.dat', 'rb') as f:\n",
    "  test_labels = pickle.load(f)\n",
    "with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/testing_data/DataSet_13_test_samples_onehot_batches5_Chr:chr1.dat', 'rb') as f:\n",
    "  test_samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "WDb1Wai7cu6a",
    "outputId": "c2e7860e-7e45-4c0c-e30d-36cca6fdd296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1\n",
      "part:1000\n",
      "chr2\n",
      "part:2000\n",
      "part:3000\n",
      "chr3\n",
      "part:4000\n",
      "part:5000\n",
      "chr4\n",
      "part:6000\n",
      "chr5\n",
      "part:7000\n",
      "chr6\n",
      "part:8000\n",
      "part:9000\n",
      "chr7\n",
      "part:10000\n",
      "chr9\n",
      "part:11000\n",
      "chr10\n",
      "part:12000\n",
      "chr11\n",
      "part:13000\n",
      "chr12\n",
      "part:14000\n",
      "part:15000\n",
      "chr13\n",
      "chr14\n",
      "part:16000\n",
      "chr15\n",
      "part:17000\n",
      "chr16\n",
      "chr17\n",
      "part:18000\n",
      "chr18\n",
      "part:19000\n",
      "chr19\n",
      "part:20000\n",
      "chr20\n",
      "chr21\n",
      "part:21000\n",
      "chr22\n",
      "chrX\n",
      "part:22000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "part = 0 \n",
    "size_batches = 5\n",
    "num_place = 0\n",
    "for x, indexes in enumerate(genome):\n",
    "  if x != 7:\n",
    "    print(indexes)\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "      train_samples = pickle.load(f1)\n",
    "      #print(len(train_samples))\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "      train_labels = pickle.load(f2)\n",
    "      #print(len(train_labels))\n",
    "    size = len(train_samples)\n",
    "    if size > size_batches:\n",
    "      loop = 0\n",
    "      while size > 0:\n",
    "        if num_place > 20000:\n",
    "          num_place -= 20000\n",
    "        if size - size_batches > 0:\n",
    "          new_train_samples = oneHot_DNA_full_PaddingZeros(train_samples[loop:(loop + size_batches)], largest_seq)\n",
    "          #print(new_train_samples.shape)\n",
    "          new_train_labels = oneHot_labels2(train_labels[loop:loop + size_batches])\n",
    "          #print(new_train_labels.shape)\n",
    "          loop += size_batches\n",
    "          size -= size_batches\n",
    "          part += 1\n",
    "          num_place += 1\n",
    "        else:\n",
    "          new_train_samples = oneHot_DNA_full_PaddingZeros(train_samples[loop:len(train_samples)], largest_seq)\n",
    "          #print(new_train_samples.shape)\n",
    "          new_train_labels = oneHot_labels2(train_labels[loop:len(train_samples)])\n",
    "          #print(new_train_labels.shape)\n",
    "          part += 1\n",
    "          loop += size_batches\n",
    "          size -= size_batches\n",
    "          num_place += 1\n",
    "        if part % 1000 == 0:\n",
    "          print('part:' + str(part))\n",
    "        with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/' + str(int(part / 20000))+'/' + str(int(num_place / 100))+'/DataSet_13_train_labels_onehot_batches5_Part:' + str(part) + '.dat', 'wb') as f:\n",
    "          pickle.dump(new_train_labels, f)\n",
    "        with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/test5/' + str(int(part / 20000))+'/' + str(int(num_place / 100))+'/DataSet_13_train_samples_onehot_batches5_Part:' + str(part) + '.dat', 'wb') as f:\n",
    "          pickle.dump(new_train_samples, f)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qu4ophZwENom"
   },
   "source": [
    "**Batches:**\n",
    "\n",
    "**Batches 1**:\n",
    "\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_labels_onehot_batches1_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_labels = pickle.load(f)\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_samples_onehot_batches1_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_samples = pickle.load(f)\n",
    "\n",
    "**Batches 200**:\n",
    "\n",
    "     with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_labels_onehot_batches200_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_labels = pickle.load(f)\n",
    "    with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_samples_onehot_batches200_Part:' + str(part) + '.dat', 'rb') as f:\n",
    "      train_samples = pickle.load(f)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HC7t10jQJi87"
   },
   "source": [
    "part:73000 last run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2uqQq493jAT"
   },
   "source": [
    "Splitting into groups of 200 bp and then one hot encoding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCLQXpAFFjId"
   },
   "outputs": [],
   "source": [
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[0] + '.dat', 'rb') as f1:\n",
    "    train_samples = pickle.load(f1)\n",
    "    #print(len(train_samples))\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[0] + '.dat', 'rb') as f2:\n",
    "    train_labels = pickle.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "5Rqk9NE7wmK-",
    "outputId": "4c5ba976-89fd-4890-b7a7-5b0b27e465bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b680ee25972b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHot_DNA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7a68d8e0ded5>\u001b[0m in \u001b[0;36moneHot_DNA\u001b[0;34m(chrom)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moneHot_DNA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mone_hot_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mseq_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "for x, indexes in enumerate(genome):\n",
    "  train_samples = []\n",
    "  train_labels = []\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "    train_samples = pickle.load(f1)\n",
    "    #print(len(train_samples))\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "    train_labels = pickle.load(f2)\n",
    "\n",
    "  ## Loop for spliting sequences and labels to 200 bp:\n",
    "  count = 0\n",
    "  new_train_seq = []\n",
    "  new_train_labels = []\n",
    "  count_train = 0\n",
    "  for seq in train_samples:\n",
    "    for i in range(0, len(seq), 200):\n",
    "      new_train_seq.append(seq[i : i+200])\n",
    "      new_train_labels.append(train_labels[count])\n",
    "    count += 1\n",
    "    count_train += 1\n",
    "  print(count_train)\n",
    "  print(len(new_train_seq))\n",
    "  print(len(new_train_labels))\n",
    "\n",
    "  ## Making sure the size is right no less than 200 bp\n",
    "  #import math\n",
    "  sample_new = []\n",
    "  label_new = []\n",
    "  for j, seq in enumerate(new_train_seq):\n",
    "    if len(seq) >= 200:\n",
    "      sample_new.append(seq)\n",
    "      label_new.append(new_train_labels[j])\n",
    "  train_samples = sample_new\n",
    "  train_labels = label_new\n",
    "  print(len(train_samples))\n",
    "  print(len(train_labels))\n",
    "  \n",
    "  train_samples = oneHot_DNA(train_samples)\n",
    "  print(train_samples.shape)\n",
    "\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_200bp_onehot/DataSet_13_train_labels_onehot_200bp' + genome[x] + '.dat', 'wb') as f:\n",
    "    pickle.dump(train_samples, f)\n",
    "  #with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_samples_onehot_200bp' + genome[x] + '.dat', 'rb') as f:\n",
    "  #  train_samples = pickle.load(f)\n",
    "  #  print(train_samples.shape)\n",
    "\n",
    "  train_labels = oneHot_labels2(train_labels)\n",
    "  print(train_labels.shape)\n",
    "\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_200bp_onehot/DataSet_13_train_labels_onehot_200bp' + genome[x] + '.dat', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "  #with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_train_labels_onehot_200bp' + genome[x] + '.dat', 'rb') as f:\n",
    "  #  train_labels = pickle.load(f)\n",
    "  #  print(train_labels.shape) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "sxfsM3Bnw5CH",
    "outputId": "948c3000-858d-4fe6-db22-a808a3ba6c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282\n",
      "23174\n",
      "1\n",
      "23152\n",
      "1\n",
      "(33572, 200, 4)\n",
      "(33572, 2)\n",
      "1046\n",
      "20874\n",
      "1\n",
      "20862\n",
      "1\n",
      "(28674, 200, 4)\n",
      "(28674, 2)\n",
      "898\n",
      "16692\n",
      "1\n",
      "16690\n",
      "1\n",
      "(23386, 200, 4)\n",
      "(23386, 2)\n",
      "734\n",
      "16066\n",
      "1\n",
      "16064\n",
      "1\n",
      "(24034, 200, 4)\n",
      "(24034, 2)\n",
      "724\n",
      "19312\n",
      "1\n",
      "19310\n",
      "1\n",
      "(25486, 200, 4)\n",
      "(25486, 2)\n",
      "806\n",
      "19240\n",
      "1\n",
      "19236\n",
      "1\n",
      "(25698, 200, 4)\n",
      "(25698, 2)\n",
      "858\n",
      "15162\n",
      "1\n",
      "15156\n",
      "1\n",
      "(31468, 200, 4)\n",
      "(31468, 2)\n",
      "646\n",
      "14822\n",
      "1\n",
      "14818\n",
      "1\n",
      "(23394, 200, 4)\n",
      "(23394, 2)\n",
      "680\n",
      "12174\n",
      "1\n",
      "12134\n",
      "1\n",
      "(15648, 200, 4)\n",
      "(15648, 2)\n",
      "670\n",
      "12136\n",
      "1\n",
      "12122\n",
      "1\n",
      "(20088, 200, 4)\n",
      "(20088, 2)\n",
      "770\n",
      "14906\n",
      "1\n",
      "14900\n",
      "1\n",
      "(26872, 200, 4)\n",
      "(26872, 2)\n",
      "764\n",
      "12766\n",
      "1\n",
      "12766\n",
      "1\n",
      "(21218, 200, 4)\n",
      "(21218, 2)\n",
      "474\n",
      "7382\n",
      "1\n",
      "7382\n",
      "1\n",
      "(10540, 200, 4)\n",
      "(10540, 2)\n",
      "466\n",
      "7840\n",
      "1\n",
      "7840\n",
      "1\n",
      "(14790, 200, 4)\n",
      "(14790, 2)\n",
      "498\n",
      "10560\n",
      "1\n",
      "10556\n",
      "1\n",
      "(16076, 200, 4)\n",
      "(16076, 2)\n",
      "486\n",
      "6324\n",
      "1\n",
      "6324\n",
      "1\n",
      "(12346, 200, 4)\n",
      "(12346, 2)\n",
      "612\n",
      "6852\n",
      "1\n",
      "6840\n",
      "1\n",
      "(9706, 200, 4)\n",
      "(9706, 2)\n",
      "302\n",
      "7772\n",
      "1\n",
      "7770\n",
      "1\n",
      "(10640, 200, 4)\n",
      "(10640, 2)\n",
      "578\n",
      "5074\n",
      "1\n",
      "5064\n",
      "1\n",
      "(19232, 200, 4)\n",
      "(19232, 2)\n",
      "390\n",
      "5964\n",
      "1\n",
      "5962\n",
      "1\n",
      "(8872, 200, 4)\n",
      "(8872, 2)\n",
      "178\n",
      "4904\n",
      "1\n",
      "4900\n",
      "1\n",
      "(8562, 200, 4)\n",
      "(8562, 2)\n",
      "264\n",
      "2550\n",
      "1\n",
      "2536\n",
      "1\n",
      "(3320, 200, 4)\n",
      "(3320, 2)\n",
      "454\n",
      "18592\n",
      "1\n",
      "18564\n",
      "1\n",
      "(27186, 200, 4)\n",
      "(27186, 2)\n"
     ]
    }
   ],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "for x, indexes in enumerate(genome):\n",
    "  test_samples = []\n",
    "  test_labels = []\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_test_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "    test_samples = pickle.load(f1)\n",
    "    #print(len(train_samples))\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_13_notonehot/DataSet_13_test_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "    test_labels = pickle.load(f2)\n",
    "  count_test = 0\n",
    "  for q in test_labels:\n",
    "    if q != 1:\n",
    "      count_test += 1\n",
    "  print(count_test)\n",
    "\n",
    "  ## Loop for spliting sequences and labels to 200 bp:\n",
    "  count = 0\n",
    "  new_test_seq = []\n",
    "  new_test_labels = []\n",
    "  count_test = 0\n",
    "  for seq in test_samples:\n",
    "    for i in range(0, len(seq), 200):\n",
    "      new_test_seq.append(seq[i : i+200])\n",
    "      new_test_labels.append(test_labels[count])\n",
    "    count += 1\n",
    "    count_test += 1\n",
    "  # print(count_test)\n",
    "  # print(len(new_test_seq))\n",
    "  # print(len(new_test_labels))\n",
    "  #    print(count_test)\n",
    "#    print(len(new_test_seq))\n",
    "#    print(len(new_test_labels))\n",
    "\n",
    "  count_test = 0\n",
    "  for q in new_test_labels:\n",
    "    if q != 1:\n",
    "      count_test += 1\n",
    "  print(count_test)\n",
    "  print(test_labels[0])\n",
    "\n",
    "\n",
    "  ## Making sure the size is right no less than 200 bp\n",
    "  #import math\n",
    "  sample_new = []\n",
    "  label_new = []\n",
    "  for i, seq in enumerate(new_test_seq):\n",
    "    if len(seq) >= 200:\n",
    "      sample_new.append(seq)\n",
    "      label_new.append(new_test_labels[i])\n",
    "  test_samples = sample_new\n",
    "  test_labels = label_new\n",
    "  # print(len(test_samples))\n",
    "  # print(len(test_labels))\n",
    "  \n",
    "  count_test = 0\n",
    "  for q in test_labels:\n",
    "    if q != 1:\n",
    "      count_test += 1\n",
    "  print(count_test)\n",
    "  print(test_labels[0])\n",
    "\n",
    "  test_samples = oneHot_DNA(test_samples)\n",
    "  print(test_samples.shape)\n",
    "\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_200bp_onehot/DataSet_13_test_samples_onehot_200bp' + genome[x] + '.dat', 'wb') as f:\n",
    "    pickle.dump(test_samples, f)\n",
    "  #with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_samples_onehot_200bp' + genome[i] + '.dat', 'rb') as f:\n",
    "  #  test_samples = pickle.load(f)\n",
    "  #  print(test_samples.shape)\n",
    "\n",
    "  test_labels = oneHot_labels2(test_labels)\n",
    "  print(test_labels.shape)\n",
    "\n",
    "  with open('gdrive/My Drive/SURF_2020_Weiss/Data_Sets/DataSet13/DataSet_200bp_onehot/DataSet_13_test_labels_onehot_200bp' + genome[x] + '.dat', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "  #with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_labels_onehot_200bp' + genome[i] + '.dat', 'rb') as f:\n",
    "  #  test_labels = pickle.load(f)\n",
    "  #  print(test_labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lPyFXxRI7Q9H",
    "outputId": "40989a2d-f13d-4c11-a1d3-e86433e31a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360\n",
      "1334\n"
     ]
    }
   ],
   "source": [
    "with open('gdrive/My Drive/SURF_2020_Weiss/E003_hg38_25_imputed12marks/DataSet_13_test_labels_' + genome[0] + '.dat', 'rb') as f2:\n",
    "  test_labels = pickle.load(f2)\n",
    "  print(len(test_labels))\n",
    "  count_test = 0\n",
    "  for q in test_labels:\n",
    "    if q != 1:\n",
    "      count_test += 1\n",
    "  print(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJxUeYbMpqN5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Creating Data Set 14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
