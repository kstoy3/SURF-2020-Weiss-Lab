{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LQ3AGvRiS6y"
   },
   "source": [
    "Binary heterochromatin classification\n",
    "- Two categories \n",
    "    1. either chromatin state 13 \n",
    "    2. or one of the other 25 categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwWEdAva_945"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, LSTM\n",
    "from keras.layers import Lambda, Convolution1D, MaxPooling1D, Flatten, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.metrics import categorical_crossentropy, binary_crossentropy\n",
    "#For data saving\n",
    "import pickle\n",
    "import random\n",
    "#other imports\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "import keras.backend as K\n",
    "import os\n",
    "#cwd = os.path.dirname(os.path.realpath(\"SURF_001_TwoClass13.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMMmIQk3Rvc"
   },
   "source": [
    "### Loading algorithims and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59vW0oPKF4NF"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-qiIbo-AZgG"
   },
   "outputs": [],
   "source": [
    "def oneHot_DNA(chrom):\n",
    "    \"\"\"\n",
    "    Given a chromosome array with a, t, c, and g's will output a one-hot encoded vector array\n",
    "    @return one-hot encoded numpy array\n",
    "    \"\"\"\n",
    "    one_hot_full = np.zeros((len(chrom), len(chrom[0]), 4), dtype=np.int8)\n",
    "    for i, seq in enumerate(chrom):\n",
    "        seq_onehot = np.zeros((len(seq), 4))\n",
    "        for j, nuc in enumerate(seq):\n",
    "            if nuc == 'a':\n",
    "                seq_onehot[j, :] = np.array([1, 0, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 't':\n",
    "                seq_onehot[j, :] = np.array([0, 1, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 'c':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 1, 0], dtype=np.int8)\n",
    "            elif nuc == 'g':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 0, 1], dtype=np.int8)\n",
    "            one_hot_full[i,:,:] = seq_onehot\n",
    "    return one_hot_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fREkD6bmc_cN"
   },
   "outputs": [],
   "source": [
    "def oneHot_DNA_full_PaddingZeros(chrom, max):\n",
    "    \"\"\" Given a chromosome array with sequences. Will pad sequeneces to max length given and will shorten any sequences longer than max\n",
    "    \n",
    "    @return one-hot encoded numpy array with Zero-padding\n",
    "    \"\"\"\n",
    "    largest_size = max\n",
    "    #for seq in chrom:\n",
    "    #  size = len(seq)\n",
    "    #  if size > largest_size:\n",
    "    #    largest_size = size\n",
    "    one_hot_full = np.zeros((len(chrom), largest_size, 4), dtype=np.int8)\n",
    "    for i, seq in enumerate(chrom):\n",
    "        seq_onehot = np.zeros((largest_size, 4))\n",
    "        for j, nuc in enumerate(seq):\n",
    "            if nuc == 'a':\n",
    "                seq_onehot[j, :] = np.array([1, 0, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 't':\n",
    "                seq_onehot[j, :] = np.array([0, 1, 0, 0], dtype=np.int8)\n",
    "            elif nuc == 'c':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 1, 0], dtype=np.int8)\n",
    "            elif nuc == 'g':\n",
    "                seq_onehot[j, :] = np.array([0, 0, 0, 1], dtype=np.int8)\n",
    "            else:\n",
    "                print('issue')\n",
    "            one_hot_full[i,:,:] = seq_onehot\n",
    "    return one_hot_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06qGFesc3gSD"
   },
   "source": [
    "### Onehot encoding the samples and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "_XRVBWpeg2NY",
    "outputId": "5cbf8478-0c71-42f3-f9e8-c3a47a5178cb"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "\n",
    "max_num = 0\n",
    "for x, indexes in enumerate(genome):\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_test_samples_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        test_samples = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_samples = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_test_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        test_labels = pickle.load(f2)\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_labels = pickle.load(f2)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        if label == 0:\n",
    "            if max_num < len(train_samples[i]):\n",
    "                max_num = len(train_samples[i])\n",
    "    for i, label in enumerate(test_labels):\n",
    "        if label == 0:\n",
    "            if max_num < len(test_samples[i]):\n",
    "                 max_num = len(test_samples[i])\n",
    "    print(max_num)\n",
    "    print(max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2b4hp3QnfMPk"
   },
   "outputs": [],
   "source": [
    "largest_seq = max_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2-icGZ0Krkkz",
    "outputId": "a30af7e1-8849-4b4e-a661-e24d55f69b0d"
   },
   "outputs": [],
   "source": [
    "count_a = 0\n",
    "count_a_place = 0\n",
    "count_b = 0\n",
    "count_b_place = 0\n",
    "count_c = 0\n",
    "count_c_place = 0\n",
    "count = 0\n",
    "\n",
    "for x, indexes in enumerate(genome):\n",
    "    train_samples = []\n",
    "    train_labels = []\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_samples_1-Version_' + genome[x] + '.dat', 'rb') as f1:\n",
    "        train_samples = pickle.load(f1)\n",
    "        #print(len(train_samples))\n",
    "    with open('../DataSet_13_notonehot/DataSet_13_train_labels_1-Version_' + genome[x] + '.dat', 'rb') as f2:\n",
    "        train_labels = pickle.load(f2)\n",
    "        #print(len(train_labels))\n",
    "    for y, index in enumerate(train_samples):\n",
    "        len_seq = len(index)\n",
    "        new_train_samples = []\n",
    "        new_train_labels = []\n",
    "        index_array = []\n",
    "        index_array.append(index)\n",
    "        if count_a_place > 10000:\n",
    "            count_a_place -= 10000\n",
    "        if count_b_place > 10000:\n",
    "            count_b_place -= 10000\n",
    "        if count_c_place > 10000:\n",
    "            count_c_place -= 10000\n",
    "        if len_seq <= 1000:\n",
    "            if count > 9990 and count < 10000:\n",
    "                new_train_samples = oneHot_DNA_full_PaddingZeros(index_array, 1000)\n",
    "                new_train_labels = oneHot_labels2_1batch(train_labels[y])\n",
    "            if count > 9990 and count < 10000:\n",
    "                print(new_train_samples.shape)\n",
    "                print(train_labels[y])\n",
    "                print(new_train_labels)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJxUeYbMpqN5"
   },
   "source": [
    "### Loading all the Data into one Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../partition_train_val_a.dat', 'rb') as f:\n",
    "    partition = pickle.load(f)\n",
    "train_samples = partition['train']\n",
    "val_samples = partition['validation']\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(val_samples)\n",
    "labels = partition['labels']\n",
    "partition['train'] = train_samples\n",
    "partition['validation'] = val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_samples))\n",
    "print(len(val_samples))\n",
    "print(len(labels))\n",
    "print(len(train_samples + val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = collate_data(train_samples, labels, '../model_11_data_4/', 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(samples, labels, path, sequence_length, num_categories):\n",
    "    num_train = len(samples)\n",
    "    Y_train = np.zeros((num_train, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((num_train, sequence_length, 4), dtype=np.int8)\n",
    "    count = 0\n",
    "    for i in range(0, num_train):\n",
    "        a = samples[count]\n",
    "        X_train[i, :, :] = np.load(path + samples[count] + '.npy') \n",
    "        Y_train[i, :] = labels[a] \n",
    "        count += 1\n",
    "    return X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_train.npy', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open('../DataSet15/y_train.npy', 'wb') as f:\n",
    "    pickle.dump(Y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = collate_data(val_samples, labels, '../model_11_data_4/', 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_val.npy', 'wb') as f:\n",
    "    pickle.dump(X_val, f)\n",
    "    print(X_val)\n",
    "with open('../DataSet15/y_val.npy', 'wb') as f:\n",
    "    pickle.dump(Y_val, f)\n",
    "    print(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_train.npy', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    print(X_train.shape)\n",
    "with open('../DataSet15/y_train.npy', 'rb') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "    print(Y_train.shape)\n",
    "\n",
    "\n",
    "with open('../DataSet15/x_val.npy', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "    print(X_val.shape)\n",
    "with open('../DataSet15/y_val.npy', 'rb') as f:\n",
    "    Y_val = pickle.load(f)\n",
    "    print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_val.shape[0])\n",
    "print(X_val.shape[0] + X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_two_arrays(x_train, y_train, x_val, y_val, sequence_length, num_categories):\n",
    "    num_train = x_train.shape[0] + x_val.shape[0]\n",
    "    print(num_train)\n",
    "    Y_train = np.zeros((num_train, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((num_train, sequence_length, 4), dtype=np.int8)\n",
    "    for i in range(0, x_train.shape[0]):\n",
    "        X_train[i, :, :] = x_train[i]\n",
    "        Y_train[i, :] = y_train[i] \n",
    "    for i in range(0, x_val.shape[0]):\n",
    "        X_train[(i + x_train.shape[0]), :, :] = x_val[i]\n",
    "        Y_train[(i + x_train.shape[0]), :] = y_val[i] \n",
    "    return X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = collate_two_arrays(X_train, Y_train, X_val, Y_val, 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[shuffler]\n",
    "Y_train = Y_train[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_train_val.npy', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    print(X_train)\n",
    "with open('../DataSet15/y_train_val.npy', 'wb') as f:\n",
    "    pickle.dump(Y_train, f)\n",
    "    print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Test and TrainVal Data sets, shuffle, and then split into two data sets\n",
    "- one for training and validation\n",
    "- second one for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_two_arrays(x_array_1, y_array_1, x_array_2, y_array_2, sequence_length, num_categories):\n",
    "    num_train = x_array_1.shape[0] + x_array_2.shape[0]\n",
    "    print(num_train)\n",
    "    Y_train = np.zeros((num_train, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((num_train, sequence_length, 4), dtype=np.int8)\n",
    "    for i in range(0,x_array_1.shape[0]):\n",
    "        X_train[i, :, :] = x_array_1[i]\n",
    "        Y_train[i, :] = y_array_1[i] \n",
    "    # minues 2 instead of one because the last value in the testing array is null for some reason so I am getting rid of it\n",
    "    for i in range(0, x_array_2.shape[0]):\n",
    "        X_train[(i + x_array_1.shape[0]), :, :] = x_array_2[i]\n",
    "        Y_train[(i + x_array_1.shape[0]), :] = y_array_2[i] \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, Y_all = collate_two_arrays(X_train, Y_train, X_test, Y_test, 1000, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.random.permutation(X_all.shape[0])\n",
    "X_all = X_all[shuffler]\n",
    "Y_all = Y_all[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_all_TRAIN-VAL-TEST.npy', 'wb') as f:\n",
    "    pickle.dump(X_all, f)\n",
    "    print(X_all)\n",
    "with open('../DataSet15/y_all_TRAIN-VAL-TEST.npy', 'wb') as f:\n",
    "    pickle.dump(Y_all, f)\n",
    "    print(Y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all.shape)\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for separating out the training and validation dataset\n",
    "def split_two_arrays(x_array_1, y_array_1, length, sequence_length, num_categories):\n",
    "    num_train = length\n",
    "    print(num_train)\n",
    "    Y_train = np.zeros((length, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((length, sequence_length, 4), dtype=np.int8)\n",
    "    for i in range(0, length):\n",
    "        X_train[i, :, :] = x_array_1[i]\n",
    "        Y_train[i, :] = y_array_1[i] \n",
    "    # minues 2 instead of one because the last value in the testing array is null for some reason so I am getting rid of it\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new, y_train_new = split_two_arrays(X_all, Y_all, 67602, 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for separating out the testing data set\n",
    "def split_two_arrays_2(x_array_1, y_array_1, length, sequence_length, num_categories):\n",
    "    num_train = length\n",
    "    print(num_train)\n",
    "    Y_train = np.zeros((16382, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((16382, sequence_length, 4), dtype=np.int8)\n",
    "    for i in range(0, 16382):\n",
    "        X_train[i, :, :] = x_array_1[i + 67602]\n",
    "        Y_train[i, :] = y_array_1[i + 67602] \n",
    "    # minues 2 instead of one because the last value in the testing array is null for some reason so I am getting rid of it\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_new, y_test_new = split_two_arrays_2(X_all, Y_all, 16382, 1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_train_allshuffled.npy', 'wb') as f:\n",
    "    pickle.dump(x_train_new, f)\n",
    "    print(x_train_new.shape)\n",
    "    print(x_train_new)\n",
    "with open('../DataSet15/y_train_allshuffled.npy', 'wb') as f:\n",
    "    pickle.dump(y_train_new, f)\n",
    "    print(y_train_new.shape)\n",
    "    print(y_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../DataSet15/x_test_allshuffled.npy', 'wb') as f:\n",
    "    pickle.dump(x_test_new, f)\n",
    "    print(x_test_new.shape)\n",
    "    print(x_test_new)\n",
    "with open('../DataSet15/y_test_allshuffled.npy', 'wb') as f:\n",
    "    pickle.dump(y_test_new, f)\n",
    "    print(y_test_new.shape)\n",
    "    print(y_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Creating Data Set 14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
