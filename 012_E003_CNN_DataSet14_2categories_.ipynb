{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoFyIizw-YaT"
   },
   "source": [
    "CNN LSTM \n",
    "\n",
    "Data Set 14\n",
    "Using data zero data zero padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISksYe7Fr3LI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input ,Dense, Dropout, Activation, LSTM, Conv1D\n",
    "from keras.layers import Lambda, Convolution1D, MaxPooling1D, Flatten, Reshape, BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.metrics import categorical_crossentropy, binary_crossentropy\n",
    "\n",
    "# In the future may add:\n",
    "#from spp.SpatialPyramidPooling import SpatialPyramidPooling\n",
    "\n",
    "#For data saving\n",
    "import pickle\n",
    "import random\n",
    "#other imports\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "import keras.backend as K\n",
    "import os\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5AyunGGYTSd"
   },
   "outputs": [],
   "source": [
    "genome = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-06FGqcExs6j"
   },
   "outputs": [],
   "source": [
    "dense_layer = 1\n",
    "layer_size = 64 \n",
    "conv_layer = 2\n",
    "seq_length = 1000\n",
    "base_pair = 4\n",
    "num_strides = 1\n",
    "pool_size = 4\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "val_batch_size = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model with all the data in one numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67602, 1000, 4)\n",
      "[[[1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "(67602, 2)\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "with open('../DataSet15/x_train_allshuffled.npy', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    print(X_train.shape)\n",
    "    print(X_train)\n",
    "with open('../DataSet15/y_train_allshuffled.npy', 'rb') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffeling the Data sets before training\n",
    "shuffler = np.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[shuffler]\n",
    "Y_train = Y_train[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('../DataSet15/x_train.npy', 'rb') as f:\\n    X_train = pickle.load(f)\\n    print(X_train.shape)\\nwith open('../DataSet15/y_train.npy', 'rb') as f:\\n    Y_train = pickle.load(f)\\n    print(Y_train.shape)\\n\\n\\nwith open('../DataSet15/x_val.npy', 'rb') as f:\\n    X_val = pickle.load(f)\\n    print(X_val.shape)\\nwith open('../DataSet15/y_val.npy', 'rb') as f:\\n    Y_val = pickle.load(f)\\n    print(Y_val.shape)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The version of Data Set with training and validation pre-split \n",
    "'''\n",
    "with open('../DataSet15/x_train.npy', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    print(X_train.shape)\n",
    "with open('../DataSet15/y_train.npy', 'rb') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "    print(Y_train.shape)\n",
    "\n",
    "\n",
    "with open('../DataSet15/x_val.npy', 'rb') as f:\n",
    "    X_val = pickle.load(f)\n",
    "    print(X_val.shape)\n",
    "with open('../DataSet15/y_val.npy', 'rb') as f:\n",
    "    Y_val = pickle.load(f)\n",
    "    print(Y_val.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Neural Network Arcitecture\n",
    "def train_CNN_full(X_train, Y_train, checkpoint_path, Conv_filter, Conv_layers, Dense_layers, Dense_filter):\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True, restore_best_weights=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "    ## Model\n",
    "    model = tf.keras.models.Sequential()             \n",
    "    model.add(tf.keras.layers.Conv1D(Conv_filter, (4), activation='relu', input_shape=(1000, 4)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "    model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    for l in range(Conv_layers):\n",
    "        model.add(tf.keras.layers.Conv1D(Conv_filter, (3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    model.add(tf.keras.layers.LSTM(64))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for l in range(Dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(Dense_filter, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))      \n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    ## Fitting Model\n",
    "    history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n",
    "                        callbacks=[earlystop, cp_callback], validation_split=0.3, \n",
    "                        validation_batch_size=val_batch_size, shuffle=True)\n",
    "\n",
    "    return history.epoch, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 997, 128)          2176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 249, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 249, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 247, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 142,466\n",
      "Trainable params: 142,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 108s - loss: 0.6336 - accuracy: 0.6261 - val_loss: 0.6196 - val_accuracy: 0.6381\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.6206 - accuracy: 0.6450 - val_loss: 0.6076 - val_accuracy: 0.6601\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.5962 - accuracy: 0.6842 - val_loss: 0.5762 - val_accuracy: 0.7027\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 104s - loss: 0.5648 - accuracy: 0.7146 - val_loss: 0.5359 - val_accuracy: 0.7409\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 113s - loss: 0.5460 - accuracy: 0.7294 - val_loss: 0.5347 - val_accuracy: 0.7436\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 107s - loss: 0.5338 - accuracy: 0.7396 - val_loss: 0.5358 - val_accuracy: 0.7351\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.5274 - accuracy: 0.7423 - val_loss: 0.5177 - val_accuracy: 0.7506\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 112s - loss: 0.5165 - accuracy: 0.7484 - val_loss: 0.5147 - val_accuracy: 0.7558\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 107s - loss: 0.5044 - accuracy: 0.7564 - val_loss: 0.4942 - val_accuracy: 0.7597\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 104s - loss: 0.4946 - accuracy: 0.7635 - val_loss: 0.4972 - val_accuracy: 0.7588\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 106s - loss: 0.4860 - accuracy: 0.7691 - val_loss: 0.4837 - val_accuracy: 0.7673\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4762 - accuracy: 0.7735 - val_loss: 0.4949 - val_accuracy: 0.7610\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 106s - loss: 0.4652 - accuracy: 0.7808 - val_loss: 0.4746 - val_accuracy: 0.7797\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4626 - accuracy: 0.7811 - val_loss: 0.4793 - val_accuracy: 0.7705\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4506 - accuracy: 0.7899 - val_loss: 0.4555 - val_accuracy: 0.7868\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4428 - accuracy: 0.7943 - val_loss: 0.4500 - val_accuracy: 0.7898\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4354 - accuracy: 0.7985 - val_loss: 0.4535 - val_accuracy: 0.7903\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 103s - loss: 0.4297 - accuracy: 0.8006 - val_loss: 0.4602 - val_accuracy: 0.7885\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 103s - loss: 0.4226 - accuracy: 0.8048 - val_loss: 0.4513 - val_accuracy: 0.7896\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 102s - loss: 0.4190 - accuracy: 0.8071 - val_loss: 0.4745 - val_accuracy: 0.7845\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 102s - loss: 0.4149 - accuracy: 0.8118 - val_loss: 0.4758 - val_accuracy: 0.7734\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.4129 - accuracy: 0.8102 - val_loss: 0.4556 - val_accuracy: 0.7961\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 110s - loss: 0.4102 - accuracy: 0.8108 - val_loss: 0.4559 - val_accuracy: 0.7884\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 107s - loss: 0.4029 - accuracy: 0.8167 - val_loss: 0.4398 - val_accuracy: 0.7957\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.3936 - accuracy: 0.8221 - val_loss: 0.4818 - val_accuracy: 0.7711\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 114s - loss: 0.3967 - accuracy: 0.8220 - val_loss: 0.4557 - val_accuracy: 0.7852\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 102s - loss: 0.3843 - accuracy: 0.8274 - val_loss: 0.4801 - val_accuracy: 0.7930\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 101s - loss: 0.3788 - accuracy: 0.8306 - val_loss: 0.4552 - val_accuracy: 0.7936\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 101s - loss: 0.3761 - accuracy: 0.8311 - val_loss: 0.4493 - val_accuracy: 0.7901\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 105s - loss: 0.3721 - accuracy: 0.8328 - val_loss: 0.4500 - val_accuracy: 0.7885\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 107s - loss: 0.3659 - accuracy: 0.8364 - val_loss: 0.4817 - val_accuracy: 0.7885\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 101s - loss: 0.3589 - accuracy: 0.8409 - val_loss: 0.4922 - val_accuracy: 0.7865\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 109s - loss: 0.3538 - accuracy: 0.8414 - val_loss: 0.4750 - val_accuracy: 0.7935\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: saving model to ../training_1\\dataset15_run8_LSTM_64__Conv-filter_128Conv_layers_1Dense_layers_3Dense_filter_128\n",
      "237/237 - 102s - loss: 0.3492 - accuracy: 0.8468 - val_loss: 0.4544 - val_accuracy: 0.7946\n"
     ]
    }
   ],
   "source": [
    "#Running the Neural Network varying the hyper parameters\n",
    "checkpoint_path = \"../training_1/dataset15_run8_LSTM_64_\"\n",
    "Conv_filter = [32, 64, 128]\n",
    "Conv_layers = [1, 2, 3]\n",
    "Dense_layers = [1, 2, 3]\n",
    "Dense_filter = [32, 64, 128, 256]\n",
    "history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[2])+ \"Conv_layers_\" + str(Conv_layers[0]) +\"Dense_layers_\" + str(Dense_layers[2]) + \"Dense_filter_\" + str(Dense_filter[2]), Conv_filter[2], Conv_layers[0], Dense_layers[2], Dense_filter[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_81 (Conv1D)           (None, 997, 32)           544       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 247, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 61, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 61, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 59, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 40,034\n",
      "Trainable params: 40,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 50s - loss: 0.6274 - accuracy: 0.6402 - val_loss: 0.5942 - val_accuracy: 0.6797\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 50s - loss: 0.5852 - accuracy: 0.6931 - val_loss: 0.5598 - val_accuracy: 0.7118\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 51s - loss: 0.5611 - accuracy: 0.7184 - val_loss: 0.5618 - val_accuracy: 0.7103\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 52s - loss: 0.5497 - accuracy: 0.7284 - val_loss: 0.5292 - val_accuracy: 0.7429\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 50s - loss: 0.5412 - accuracy: 0.7336 - val_loss: 0.5519 - val_accuracy: 0.7156\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 50s - loss: 0.5407 - accuracy: 0.7332 - val_loss: 0.5224 - val_accuracy: 0.7452\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 52s - loss: 0.5309 - accuracy: 0.7409 - val_loss: 0.5355 - val_accuracy: 0.7302\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 51s - loss: 0.5249 - accuracy: 0.7446 - val_loss: 0.5147 - val_accuracy: 0.7509\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_2\\dataset15_run8_LSTM_64__Conv-filter_32Conv_layers_2Dense_layers_2Dense_filter_64\n",
      "237/237 - 52s - loss: 0.5174 - accuracy: 0.7505 - val_loss: 0.5117 - val_accuracy: 0.7567\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-5cedb89f78d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mDense_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mConv_filter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_CNN_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_Conv-filter_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"Conv_layers_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"Dense_layers_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Dense_filter_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense_filter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense_filter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mConv_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(i)\n",
      "\u001b[1;32m<ipython-input-60-fadec650fb57>\u001b[0m in \u001b[0;36mtrain_CNN_full\u001b[1;34m(X_train, Y_train, checkpoint_path, Conv_filter, Conv_layers, Dense_layers, Dense_filter)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m## Fitting Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcp_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                         validation_batch_size=val_batch_size, shuffle=True)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running with the Neural Network with a second set of hyper parameters \n",
    "checkpoint_path = \"../training_2/dataset15_run8_LSTM_64_\"\n",
    "Conv_filter = [32, 64, 128]\n",
    "Conv_layers = [1, 2, 3]\n",
    "Dense_layers = [1, 2, 3]\n",
    "Dense_filter = [32, 64, 128, 256]\n",
    "for i in Conv_filter:\n",
    "    history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(i)+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]), i, Conv_layers[1], Dense_layers[1], Dense_filter[1])\n",
    "for i in Conv_layers:\n",
    "    history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(i)\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]), Conv_filter[1], i, Dense_layers[1], Dense_filter[1])\n",
    "for i in Dense_layers:\n",
    "    history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(i) + \"Dense_filter_\" + str(Dense_filter[1]), Conv_filter[1], Conv_layers[1], i, Dense_filter[1])\n",
    "for i in Dense_filter:\n",
    "    history, model = train_CNN_full(X_train, Y_train, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(i), Conv_filter[1], Conv_layers[1], Dense_layers[1], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 997, 64)           1088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 247, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 55,042\n",
      "Trainable params: 55,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 107s - loss: 0.6199 - accuracy: 0.6475 - val_loss: 0.5908 - val_accuracy: 0.6877\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 107s - loss: 0.5882 - accuracy: 0.6925 - val_loss: 0.5603 - val_accuracy: 0.7245\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 106s - loss: 0.5567 - accuracy: 0.7213 - val_loss: 0.5436 - val_accuracy: 0.7355\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 108s - loss: 0.5415 - accuracy: 0.7336 - val_loss: 0.5404 - val_accuracy: 0.7316\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 109s - loss: 0.5384 - accuracy: 0.7361 - val_loss: 0.5680 - val_accuracy: 0.7049\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 108s - loss: 0.5244 - accuracy: 0.7438 - val_loss: 0.5336 - val_accuracy: 0.7370\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 110s - loss: 0.5211 - accuracy: 0.7498 - val_loss: 0.5117 - val_accuracy: 0.7499\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 108s - loss: 0.5108 - accuracy: 0.7549 - val_loss: 0.5010 - val_accuracy: 0.7600\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.5081 - accuracy: 0.7546 - val_loss: 0.5146 - val_accuracy: 0.7546\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 105s - loss: 0.4948 - accuracy: 0.7646 - val_loss: 0.4946 - val_accuracy: 0.7621\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 105s - loss: 0.4946 - accuracy: 0.7641 - val_loss: 0.4891 - val_accuracy: 0.7682\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 105s - loss: 0.4861 - accuracy: 0.7699 - val_loss: 0.4856 - val_accuracy: 0.7722\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4711 - accuracy: 0.7774 - val_loss: 0.4860 - val_accuracy: 0.7656\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4659 - accuracy: 0.7825 - val_loss: 0.4688 - val_accuracy: 0.7830\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4583 - accuracy: 0.7864 - val_loss: 0.4620 - val_accuracy: 0.7844\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4554 - accuracy: 0.7877 - val_loss: 0.4809 - val_accuracy: 0.7764\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4543 - accuracy: 0.7865 - val_loss: 0.4606 - val_accuracy: 0.7819\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4471 - accuracy: 0.7934 - val_loss: 0.4552 - val_accuracy: 0.7868\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4436 - accuracy: 0.7939 - val_loss: 0.4565 - val_accuracy: 0.7871\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4413 - accuracy: 0.7963 - val_loss: 0.4596 - val_accuracy: 0.7836\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4345 - accuracy: 0.7990 - val_loss: 0.4524 - val_accuracy: 0.7888\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4339 - accuracy: 0.8011 - val_loss: 0.5254 - val_accuracy: 0.7388\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4345 - accuracy: 0.8000 - val_loss: 0.4774 - val_accuracy: 0.7812\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.4298 - accuracy: 0.8031 - val_loss: 0.4610 - val_accuracy: 0.7893\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4272 - accuracy: 0.8056 - val_loss: 0.4571 - val_accuracy: 0.7867\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 102s - loss: 0.4248 - accuracy: 0.8070 - val_loss: 0.4578 - val_accuracy: 0.7856\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4193 - accuracy: 0.8092 - val_loss: 0.4484 - val_accuracy: 0.7913\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.4206 - accuracy: 0.8078 - val_loss: 0.4858 - val_accuracy: 0.7662\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4177 - accuracy: 0.8097 - val_loss: 0.4848 - val_accuracy: 0.7755\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4170 - accuracy: 0.8111 - val_loss: 0.4712 - val_accuracy: 0.7829\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4131 - accuracy: 0.8108 - val_loss: 0.5093 - val_accuracy: 0.7644\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4169 - accuracy: 0.8125 - val_loss: 0.4506 - val_accuracy: 0.7945\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.4104 - accuracy: 0.8154 - val_loss: 0.4563 - val_accuracy: 0.7887\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.4059 - accuracy: 0.8155 - val_loss: 0.4554 - val_accuracy: 0.7937\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.4060 - accuracy: 0.8169 - val_loss: 0.4521 - val_accuracy: 0.7916\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4043 - accuracy: 0.8189 - val_loss: 0.4422 - val_accuracy: 0.7944\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.4025 - accuracy: 0.8195 - val_loss: 0.4589 - val_accuracy: 0.7890\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 104s - loss: 0.3971 - accuracy: 0.8220 - val_loss: 0.4915 - val_accuracy: 0.7697\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 105s - loss: 0.3949 - accuracy: 0.8236 - val_loss: 0.4637 - val_accuracy: 0.7890\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 137s - loss: 0.3940 - accuracy: 0.8228 - val_loss: 0.4571 - val_accuracy: 0.7916\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 138s - loss: 0.3930 - accuracy: 0.8253 - val_loss: 0.4571 - val_accuracy: 0.7900\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 131s - loss: 0.3934 - accuracy: 0.8242 - val_loss: 0.4663 - val_accuracy: 0.7780\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 125s - loss: 0.3905 - accuracy: 0.8263 - val_loss: 0.4480 - val_accuracy: 0.7944\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 105s - loss: 0.3893 - accuracy: 0.8265 - val_loss: 0.4546 - val_accuracy: 0.7883\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.3885 - accuracy: 0.8274 - val_loss: 0.4553 - val_accuracy: 0.7932\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: saving model to ../training_1\\dataset15_run5_LSTM-64_ckpy_\n",
      "237/237 - 103s - loss: 0.3889 - accuracy: 0.8259 - val_loss: 0.4498 - val_accuracy: 0.7937\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 997, 64)           1088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 247, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 129,026\n",
      "Trainable params: 129,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 132s - loss: 0.6208 - accuracy: 0.6464 - val_loss: 0.6384 - val_accuracy: 0.6406\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 129s - loss: 0.5913 - accuracy: 0.6894 - val_loss: 0.5693 - val_accuracy: 0.7174\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 129s - loss: 0.5681 - accuracy: 0.7118 - val_loss: 0.5629 - val_accuracy: 0.7172\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 131s - loss: 0.5471 - accuracy: 0.7306 - val_loss: 0.5549 - val_accuracy: 0.7211\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.5356 - accuracy: 0.7376 - val_loss: 0.5246 - val_accuracy: 0.7455\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 128s - loss: 0.5278 - accuracy: 0.7445 - val_loss: 0.5177 - val_accuracy: 0.7497\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 125s - loss: 0.5207 - accuracy: 0.7489 - val_loss: 0.5126 - val_accuracy: 0.7515\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.5073 - accuracy: 0.7556 - val_loss: 0.5120 - val_accuracy: 0.7576\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.5006 - accuracy: 0.7624 - val_loss: 0.4963 - val_accuracy: 0.7650\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 128s - loss: 0.4957 - accuracy: 0.7623 - val_loss: 0.5194 - val_accuracy: 0.7514\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.4886 - accuracy: 0.7650 - val_loss: 0.5009 - val_accuracy: 0.7610\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.4812 - accuracy: 0.7721 - val_loss: 0.5532 - val_accuracy: 0.7140\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.4759 - accuracy: 0.7780 - val_loss: 0.4727 - val_accuracy: 0.7792\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 127s - loss: 0.4701 - accuracy: 0.7788 - val_loss: 0.4760 - val_accuracy: 0.7783\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 143s - loss: 0.4606 - accuracy: 0.7855 - val_loss: 0.4706 - val_accuracy: 0.7823\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 160s - loss: 0.4564 - accuracy: 0.7875 - val_loss: 0.4768 - val_accuracy: 0.7749\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 167s - loss: 0.4552 - accuracy: 0.7885 - val_loss: 0.4602 - val_accuracy: 0.7850\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 167s - loss: 0.4478 - accuracy: 0.7925 - val_loss: 0.4646 - val_accuracy: 0.7843\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4431 - accuracy: 0.7959 - val_loss: 0.4583 - val_accuracy: 0.7833\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 167s - loss: 0.4417 - accuracy: 0.7952 - val_loss: 0.4613 - val_accuracy: 0.7853\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4397 - accuracy: 0.7972 - val_loss: 0.4693 - val_accuracy: 0.7844\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4392 - accuracy: 0.7982 - val_loss: 0.4683 - val_accuracy: 0.7861\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4330 - accuracy: 0.8022 - val_loss: 0.4572 - val_accuracy: 0.7893\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4294 - accuracy: 0.8028 - val_loss: 0.4510 - val_accuracy: 0.7917\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4262 - accuracy: 0.8040 - val_loss: 0.4486 - val_accuracy: 0.7911\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4255 - accuracy: 0.8052 - val_loss: 0.4529 - val_accuracy: 0.7883\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4217 - accuracy: 0.8079 - val_loss: 0.4495 - val_accuracy: 0.7913\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4161 - accuracy: 0.8113 - val_loss: 0.4496 - val_accuracy: 0.7901\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4201 - accuracy: 0.8084 - val_loss: 0.4647 - val_accuracy: 0.7861\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 164s - loss: 0.4134 - accuracy: 0.8129 - val_loss: 0.4461 - val_accuracy: 0.7908\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4173 - accuracy: 0.8103 - val_loss: 0.4527 - val_accuracy: 0.7902\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4099 - accuracy: 0.8131 - val_loss: 0.4562 - val_accuracy: 0.7836\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 167s - loss: 0.4078 - accuracy: 0.8152 - val_loss: 0.4436 - val_accuracy: 0.7924\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4057 - accuracy: 0.8159 - val_loss: 0.4476 - val_accuracy: 0.7916\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 165s - loss: 0.4027 - accuracy: 0.8176 - val_loss: 0.4448 - val_accuracy: 0.7912\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4002 - accuracy: 0.8201 - val_loss: 0.4540 - val_accuracy: 0.7904\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 166s - loss: 0.4005 - accuracy: 0.8199 - val_loss: 0.4551 - val_accuracy: 0.7934\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 146s - loss: 0.3967 - accuracy: 0.8204 - val_loss: 0.4616 - val_accuracy: 0.7839\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 119s - loss: 0.3928 - accuracy: 0.8242 - val_loss: 0.4536 - val_accuracy: 0.7924\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 121s - loss: 0.3942 - accuracy: 0.8234 - val_loss: 0.4441 - val_accuracy: 0.7946\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 99s - loss: 0.3915 - accuracy: 0.8236 - val_loss: 0.4511 - val_accuracy: 0.7938\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 74s - loss: 0.3891 - accuracy: 0.8245 - val_loss: 0.4583 - val_accuracy: 0.7891\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: saving model to ../training_1\\dataset15_run5_LSTM-128_ckpy_\n",
      "237/237 - 71s - loss: 0.3835 - accuracy: 0.8285 - val_loss: 0.4442 - val_accuracy: 0.7951\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 997, 64)           1088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 247, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               328704    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 375,298\n",
      "Trainable params: 375,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.6310 - accuracy: 0.6330 - val_loss: 0.6372 - val_accuracy: 0.6385\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.5901 - accuracy: 0.6887 - val_loss: 0.5586 - val_accuracy: 0.7261\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.5621 - accuracy: 0.7171 - val_loss: 0.5521 - val_accuracy: 0.7335\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.5435 - accuracy: 0.7326 - val_loss: 0.5279 - val_accuracy: 0.7463\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.5335 - accuracy: 0.7401 - val_loss: 0.5274 - val_accuracy: 0.7455\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.5277 - accuracy: 0.7435 - val_loss: 0.5468 - val_accuracy: 0.7329\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.5184 - accuracy: 0.7504 - val_loss: 0.5199 - val_accuracy: 0.7460\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 132s - loss: 0.5125 - accuracy: 0.7520 - val_loss: 0.5126 - val_accuracy: 0.7473\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.5052 - accuracy: 0.7591 - val_loss: 0.5055 - val_accuracy: 0.7564\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.4987 - accuracy: 0.7632 - val_loss: 0.5008 - val_accuracy: 0.7605\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 126s - loss: 0.4927 - accuracy: 0.7650 - val_loss: 0.4923 - val_accuracy: 0.7627\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4909 - accuracy: 0.7660 - val_loss: 0.4903 - val_accuracy: 0.7622\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 131s - loss: 0.4818 - accuracy: 0.7716 - val_loss: 0.4874 - val_accuracy: 0.7638\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 131s - loss: 0.4746 - accuracy: 0.7753 - val_loss: 0.4742 - val_accuracy: 0.7797\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 137s - loss: 0.4670 - accuracy: 0.7801 - val_loss: 0.4856 - val_accuracy: 0.7693\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.4612 - accuracy: 0.7840 - val_loss: 0.4785 - val_accuracy: 0.7746\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 130s - loss: 0.4564 - accuracy: 0.7865 - val_loss: 0.4747 - val_accuracy: 0.7789\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 132s - loss: 0.4485 - accuracy: 0.7898 - val_loss: 0.4653 - val_accuracy: 0.7821\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4472 - accuracy: 0.7916 - val_loss: 0.4617 - val_accuracy: 0.7863\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4479 - accuracy: 0.7927 - val_loss: 0.4673 - val_accuracy: 0.7813\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4400 - accuracy: 0.7972 - val_loss: 0.4688 - val_accuracy: 0.7871\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 132s - loss: 0.4352 - accuracy: 0.8009 - val_loss: 0.4569 - val_accuracy: 0.7861\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4313 - accuracy: 0.8018 - val_loss: 0.4577 - val_accuracy: 0.7888\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 126s - loss: 0.4275 - accuracy: 0.8045 - val_loss: 0.4640 - val_accuracy: 0.7869\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.4294 - accuracy: 0.8047 - val_loss: 0.4506 - val_accuracy: 0.7933\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 130s - loss: 0.4263 - accuracy: 0.8040 - val_loss: 0.4601 - val_accuracy: 0.7882\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 134s - loss: 0.4228 - accuracy: 0.8081 - val_loss: 0.4736 - val_accuracy: 0.7881\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 137s - loss: 0.4159 - accuracy: 0.8114 - val_loss: 0.4518 - val_accuracy: 0.7880\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 134s - loss: 0.4357 - accuracy: 0.7990 - val_loss: 0.4577 - val_accuracy: 0.7858\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 126s - loss: 0.4278 - accuracy: 0.8023 - val_loss: 0.4529 - val_accuracy: 0.7851\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.4172 - accuracy: 0.8111 - val_loss: 0.4507 - val_accuracy: 0.7906\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 128s - loss: 0.4124 - accuracy: 0.8131 - val_loss: 0.4534 - val_accuracy: 0.7900\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 129s - loss: 0.4082 - accuracy: 0.8167 - val_loss: 0.4673 - val_accuracy: 0.7749\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 127s - loss: 0.4056 - accuracy: 0.8158 - val_loss: 0.4555 - val_accuracy: 0.7875\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: saving model to ../training_1\\dataset15_run5_LSTM-256_ckpy_\n",
      "237/237 - 129s - loss: 0.4097 - accuracy: 0.8137 - val_loss: 0.4591 - val_accuracy: 0.7870\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 997, 64)           1088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 247, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 61, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 512)               1181696   \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,261,058\n",
      "Trainable params: 1,261,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 325s - loss: 0.6467 - accuracy: 0.6144 - val_loss: 0.6124 - val_accuracy: 0.6552\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 321s - loss: 0.6167 - accuracy: 0.6524 - val_loss: 0.5835 - val_accuracy: 0.6955\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 326s - loss: 0.5772 - accuracy: 0.6997 - val_loss: 0.5586 - val_accuracy: 0.7261\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 340s - loss: 0.5558 - accuracy: 0.7218 - val_loss: 0.5770 - val_accuracy: 0.7057\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 347s - loss: 0.5429 - accuracy: 0.7313 - val_loss: 0.5464 - val_accuracy: 0.7330\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 338s - loss: 0.5417 - accuracy: 0.7358 - val_loss: 0.5424 - val_accuracy: 0.7321\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 336s - loss: 0.5335 - accuracy: 0.7361 - val_loss: 0.5486 - val_accuracy: 0.7312\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 338s - loss: 0.5291 - accuracy: 0.7399 - val_loss: 0.5211 - val_accuracy: 0.7485\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 325s - loss: 0.5210 - accuracy: 0.7474 - val_loss: 0.5326 - val_accuracy: 0.7392\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 334s - loss: 0.5170 - accuracy: 0.7486 - val_loss: 0.5105 - val_accuracy: 0.7537\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 340s - loss: 0.5137 - accuracy: 0.7516 - val_loss: 0.5199 - val_accuracy: 0.7429\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 347s - loss: 0.5010 - accuracy: 0.7619 - val_loss: 0.4937 - val_accuracy: 0.7637\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 342s - loss: 0.4887 - accuracy: 0.7678 - val_loss: 0.4864 - val_accuracy: 0.7702\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 339s - loss: 0.4884 - accuracy: 0.7670 - val_loss: 0.4882 - val_accuracy: 0.7709\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 344s - loss: 0.4765 - accuracy: 0.7768 - val_loss: 0.4860 - val_accuracy: 0.7695\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 339s - loss: 0.4724 - accuracy: 0.7775 - val_loss: 0.4879 - val_accuracy: 0.7706\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 354s - loss: 0.4709 - accuracy: 0.7813 - val_loss: 0.4858 - val_accuracy: 0.7759\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 361s - loss: 0.4596 - accuracy: 0.7856 - val_loss: 0.4718 - val_accuracy: 0.7806\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 356s - loss: 0.4591 - accuracy: 0.7854 - val_loss: 0.5199 - val_accuracy: 0.7387\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 371s - loss: 0.4586 - accuracy: 0.7866 - val_loss: 0.4908 - val_accuracy: 0.7644\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 374s - loss: 0.4486 - accuracy: 0.7924 - val_loss: 0.4671 - val_accuracy: 0.7785\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 369s - loss: 0.4448 - accuracy: 0.7948 - val_loss: 0.4672 - val_accuracy: 0.7799\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 377s - loss: 0.4356 - accuracy: 0.8000 - val_loss: 0.4590 - val_accuracy: 0.7829\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 393s - loss: 0.4379 - accuracy: 0.7986 - val_loss: 0.4912 - val_accuracy: 0.7629\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 373s - loss: 0.4293 - accuracy: 0.8033 - val_loss: 0.4562 - val_accuracy: 0.7885\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 403s - loss: 0.4301 - accuracy: 0.8043 - val_loss: 0.4489 - val_accuracy: 0.7880\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 388s - loss: 0.4284 - accuracy: 0.8041 - val_loss: 0.4651 - val_accuracy: 0.7806\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 368s - loss: 0.4217 - accuracy: 0.8081 - val_loss: 0.4790 - val_accuracy: 0.7731\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 375s - loss: 0.4210 - accuracy: 0.8080 - val_loss: 0.4578 - val_accuracy: 0.7848\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: saving model to ../training_1\\dataset15_run5_LSTM-512_ckpy_\n",
      "237/237 - 372s - loss: 0.4198 - accuracy: 0.8095 - val_loss: 0.4484 - val_accuracy: 0.7913\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-a1eece81054b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mLSTM_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mLSTM_array\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_CNN_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_ckpy_\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-3629ed3f0da3>\u001b[0m in \u001b[0;36mtrain_CNN_full\u001b[1;34m(X_train, Y_train, checkpoint_path, LSTM)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m## Fitting Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n\u001b[0m\u001b[0;32m     43\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcp_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                         validation_batch_size=val_batch_size, shuffle=True)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Varying the size of the LSTM layer Only\n",
    "checkpoint_path = \"../training_1/dataset15_run5_LSTM-\"\n",
    "LSTM_array = [64, 128, 256, 512, 1024]\n",
    "for i in LSTM_array:\n",
    "    history, model = train_CNN_full(X_train, Y_train, checkpoint_path + str(i + \"_ckpy_\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tFLDeml38Fff"
   },
   "source": [
    "### Testing which model to pick before running the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67602, 1000, 4)\n",
      "[[[1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "(67602, 2)\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "with open('../DataSet15/x_train_allshuffled.npy', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "    print(X_train.shape)\n",
    "    print(X_train)\n",
    "with open('../DataSet15/y_train_allshuffled.npy', 'rb') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.random.permutation(X_train.shape[0])\n",
    "X_train = X_train[shuffler]\n",
    "Y_train = Y_train[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_two_arrays(x_array_1, y_array_1, length, sequence_length, num_categories):\n",
    "    num_train = 20280\n",
    "    print(num_train)\n",
    "    Y_train = np.zeros((length, num_categories), dtype=np.int8)\n",
    "    X_train = np.zeros((length, sequence_length, 4), dtype=np.int8)\n",
    "    for i in range(0, 20280):\n",
    "        X_train[i, :, :] = x_array_1[i]\n",
    "        Y_train[i, :] = y_array_1[i] \n",
    "    # minues 2 instead of one because the last value in the testing array is null for some reason so I am getting rid of it\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20280\n",
      "(20280, 1000, 4)\n",
      "(20280, 2)\n",
      "[[[0 0 0 1]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 1]\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 1 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = split_two_arrays(X_train, Y_train, 20280, 1000, 2)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_val)\n",
    "print(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used if needed the Model Architecure when loading saved models \n",
    "def create_model(X_train, Y_train, checkpoint_path, Conv_filter, Conv_layers, Dense_layers, Dense_filter):\n",
    "    ## Callbacks\n",
    "    '''\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('loss')>0.99):\n",
    "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "    callback = myCallback()\n",
    "    '''\n",
    "    \n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True, restore_best_weights=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "    \n",
    "    ## Model\n",
    "    model = tf.keras.models.Sequential()             \n",
    "    model.add(tf.keras.layers.Conv1D(Conv_filter, (4), activation='relu', input_shape=(1000, 4)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "    model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    for l in range(Conv_layers):\n",
    "        model.add(tf.keras.layers.Conv1D(Conv_filter, (3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    #model.add(tf.keras.layers.LSTM(64))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for l in range(Dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(Dense_filter, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))      \n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    ## Fitting Model\n",
    "    #history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n",
    "    #                    callbacks=[earlystop, cp_callback], validation_split=0.3, \n",
    "    #                    validation_batch_size=val_batch_size, shuffle=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_filer: 32\n",
      "634/634 - 5s - loss: 0.4346 - accuracy: 0.7957\n",
      "Restored model, accuracy: 79.57%\n",
      "WARNING:tensorflow:From <ipython-input-8-a1c39cf58063>:49: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "1\n",
      "20280\n",
      "[[8120 1301]\n",
      " [2843 8016]]\n",
      "Conv_filer: 64\n",
      "634/634 - 9s - loss: 0.3866 - accuracy: 0.8300\n",
      "Restored model, accuracy: 83.00%\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "1\n",
      "20280\n",
      "[[8401 1020]\n",
      " [2428 8431]]\n",
      "Conv_filer: 128\n",
      "634/634 - 16s - loss: 0.3015 - accuracy: 0.8851\n",
      "Restored model, accuracy: 88.51%\n",
      "1\n",
      "20280\n",
      "[[8411 1010]\n",
      " [1321 9538]]\n",
      "Conv_layer: 1\n",
      "634/634 - 8s - loss: 0.3250 - accuracy: 0.8767\n",
      "Restored model, accuracy: 87.67%\n",
      "1\n",
      "20280\n",
      "[[7983 1438]\n",
      " [1063 9796]]\n",
      "Conv_layer: 2\n",
      "634/634 - 8s - loss: 0.3866 - accuracy: 0.8300\n",
      "Restored model, accuracy: 83.00%\n",
      "1\n",
      "20280\n",
      "[[8401 1020]\n",
      " [2428 8431]]\n",
      "Conv_layer: 3\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "634/634 - 9s - loss: 0.3861 - accuracy: 0.8314\n",
      "Restored model, accuracy: 83.14%\n",
      "1\n",
      "20280\n",
      "[[7863 1558]\n",
      " [1862 8997]]\n",
      "Dense_layers: 1\n",
      "634/634 - 8s - loss: 0.3936 - accuracy: 0.8248\n",
      "Restored model, accuracy: 82.48%\n",
      "1\n",
      "20280\n",
      "[[6988 2433]\n",
      " [1121 9738]]\n",
      "Dense_layers: 2\n",
      "634/634 - 12s - loss: 0.3866 - accuracy: 0.8300\n",
      "Restored model, accuracy: 83.00%\n",
      "1\n",
      "20280\n",
      "[[8401 1020]\n",
      " [2428 8431]]\n",
      "Dense_layers: 3\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "634/634 - 12s - loss: 0.3545 - accuracy: 0.8512\n",
      "Restored model, accuracy: 85.12%\n",
      "1\n",
      "20280\n",
      "[[7678 1743]\n",
      " [1275 9584]]\n",
      "Dense_Layers: 32\n",
      "634/634 - 11s - loss: 0.3729 - accuracy: 0.8409\n",
      "Restored model, accuracy: 84.09%\n",
      "1\n",
      "20280\n",
      "[[8215 1206]\n",
      " [2020 8839]]\n",
      "Dense_Layers: 64\n",
      "634/634 - 11s - loss: 0.3866 - accuracy: 0.8300\n",
      "Restored model, accuracy: 83.00%\n",
      "1\n",
      "20280\n",
      "[[8401 1020]\n",
      " [2428 8431]]\n",
      "Dense_Layers: 128\n",
      "634/634 - 11s - loss: 0.3541 - accuracy: 0.8524\n",
      "Restored model, accuracy: 85.24%\n",
      "1\n",
      "20280\n",
      "[[7503 1918]\n",
      " [1076 9783]]\n",
      "Dense_Layers: 256\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "634/634 - 12s - loss: 0.3466 - accuracy: 0.8575\n",
      "Restored model, accuracy: 85.75%\n",
      "1\n",
      "20280\n",
      "[[8070 1351]\n",
      " [1538 9321]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "# Going through all of the tested model variations and comparing validation loss and accuracy\n",
    "\n",
    "checkpoint_path = \"../training_1/dataset15_run7_LSTM_64_\"\n",
    "Conv_filter = [32, 64, 128]\n",
    "Conv_layers = [1, 2, 3]\n",
    "Dense_layers = [1, 2, 3]\n",
    "Dense_filter = [32, 64, 128, 256]\n",
    "for i in Conv_filter:\n",
    "    print(\"Conv_filer: \" + str(i))\n",
    "    #latest = tf.train.latest_checkpoint(checkpoint_path + \"_Conv-filter_\" + str(i)+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "    #                                +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]))\n",
    "    #print(latest)\n",
    "    # Create a new model instance\n",
    "    model = create_model(X_val, Y_val, checkpoint_path + \"_Conv-filter_\" + str(i)+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]), i, Conv_layers[1], Dense_layers[1], Dense_filter[1])\n",
    "    # 1Load the previously saved weights\n",
    "   # model.load_weights(latest)\n",
    "    model.load_weights(checkpoint_path + \"_Conv-filter_\" + str(i)+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]))\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(X_val,  Y_val, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "     \n",
    "    predicted = (model.predict_classes(X_val))\n",
    "    \n",
    "    expected = []\n",
    " \n",
    "    for label in Y_val:\n",
    "        expected.append(label[1])\n",
    "    print(expected[0])\n",
    "    print(len(expected))\n",
    "\n",
    "    results = confusion_matrix(expected, predicted)\n",
    "    print(results)\n",
    "\n",
    "for i in Conv_layers:\n",
    "    print(\"Conv_layer: \" + str(i))\n",
    "    #latest = tf.train.latest_checkpoint(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(i)\n",
    "    #                                +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1])+\".data-00000-of-00001\")\n",
    "    #print(latest)\n",
    "    # Create a new model instance\n",
    "    model = create_model(X_val, Y_val, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(i)\n",
    "                                        +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]), Conv_filter[1], i, Dense_layers[1], Dense_filter[1])\n",
    "    # 2Load the previously saved weights\n",
    "    model.load_weights(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(i)\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(Dense_filter[1]))\n",
    "\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(X_val,  Y_val, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "    \n",
    "    predicted = (model.predict_classes(X_val))\n",
    "    \n",
    "    expected = []\n",
    " \n",
    "    for label in Y_val:\n",
    "        expected.append(label[1])\n",
    "    print(expected[0])\n",
    "    print(len(expected))\n",
    "\n",
    "    results = confusion_matrix(expected, predicted)\n",
    "    print(results)\n",
    "\n",
    "for i in Dense_layers:\n",
    "    print(\"Dense_layers: \" + str(i))\n",
    "    #latest = tf.train.latest_checkpoint(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "    #                                +\"Dense_layers_\" + str(i) + \"Dense_filter_\" + str(Dense_filter[1])+\".data-00000-of-00001\")\n",
    "    #print(latest)\n",
    "    # Create a new model instance\n",
    "    model = create_model(X_val, Y_val, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(i) + \"Dense_filter_\" + str(Dense_filter[1]), Conv_filter[1], Conv_layers[1], i, Dense_filter[1])\n",
    "\n",
    "    # 3Load the previously saved weights\n",
    "    model.load_weights(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(i) + \"Dense_filter_\" + str(Dense_filter[1]))\n",
    "\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(X_val,  Y_val, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "    #history,\n",
    "    predicted = (model.predict_classes(X_val))\n",
    "    \n",
    "    expected = []\n",
    " \n",
    "    for label in Y_val:\n",
    "        expected.append(label[1])\n",
    "    print(expected[0])\n",
    "    print(len(expected))\n",
    "\n",
    "    results = confusion_matrix(expected, predicted)\n",
    "    print(results)\n",
    "\n",
    "for i in Dense_filter:\n",
    "    print(\"Dense_filter: \" + str(i))\n",
    "    #latest = tf.train.latest_checkpoint(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "    #                                +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(i)+\".data-00000-of-00001\")\n",
    "    #print(latest)\n",
    "    # Create a new model instance\n",
    "    model = create_model(X_val, Y_val, checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(i), Conv_filter[1], Conv_layers[1], Dense_layers[1], i)\n",
    "\n",
    "    # 4Load the previously saved weights\n",
    "    model.load_weights(checkpoint_path + \"_Conv-filter_\" + str(Conv_filter[1])+ \"Conv_layers_\" + str(Conv_layers[1])\n",
    "                                    +\"Dense_layers_\" + str(Dense_layers[1]) + \"Dense_filter_\" + str(i))\n",
    "\n",
    "    # Re-evaluate the model\n",
    "    loss, acc = model.evaluate(X_val,  Y_val, verbose=2)\n",
    "    print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "    #history, \n",
    "    predicted = (model.predict_classes(X_val))\n",
    "    \n",
    "    expected = []\n",
    " \n",
    "    for label in Y_val:\n",
    "        expected.append(label[1])\n",
    "    print(expected[0])\n",
    "    print(len(expected))\n",
    "\n",
    "    results = confusion_matrix(expected, predicted)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the models one by one if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLBMGfufTtBE"
   },
   "outputs": [],
   "source": [
    "with open('../partition_train_val_a.dat', 'rb') as f:\n",
    "    partition = pickle.load(f)\n",
    "train_samples_IDs = partition['train']\n",
    "val_samples_IDs = partition['validation']\n",
    "IDs = partition['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPfOnM6TT1yl"
   },
   "outputs": [],
   "source": [
    "predicted = []\n",
    "Y_val_expected = []\n",
    "count_input_val = 0\n",
    "num_val_seq = len(val_samples)\n",
    "done = False\n",
    "while count_input_val < num_val_seq and not(done):\n",
    "    X_val = np.zeros((1, 1000, 4), dtype=np.int8)\n",
    "    if count_input_val < (num_val_seq - 1):\n",
    "        for i in range(0, 1):\n",
    "            a = val_samples[count_input_val]\n",
    "            X_val[i, :, :] = np.load('../model_11_data_4/' + val_samples[count_input_val] + '.npy') \n",
    "            count_input_val += 1\n",
    "        predict = (model.predict_classes(X_val))\n",
    "        for label in predict:\n",
    "            predicted.append(label)\n",
    "    else:\n",
    "        done = True\n",
    "    if count_input_val == 16200:\n",
    "        done = True\n",
    "    print(count_input_val)\n",
    "print(len(predicted))\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = []\n",
    "\n",
    "for label in val_samples:\n",
    "    expected.append(labels[label][0][0])\n",
    "print(expected[0])\n",
    "print(len(expected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for data visualizaiton\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "results = confusion_matrix(expected[0:13400], predicted)\n",
    "print(results)\n",
    "# Printing the precision and recall, among other metrics\n",
    "#print(metrics.classification_report(y_act, y_pred, labels=[\"a\", \n",
    "#\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Heat map for data visulaizaiton\n",
    "sns.heatmap(results/np.sum(results), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data for Final Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16382, 1000, 4)\n",
      "[[[0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 1 0]\n",
      "  ...\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]\n",
      "  [0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 1 0 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 1]\n",
      "  [0 1 0 0]\n",
      "  [0 0 0 1]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 1 0 0]\n",
      "  ...\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]\n",
      "  [0 0 0 0]]]\n",
      "(16382, 2)\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "with open('../DataSet15/x_test_allshuffled.npy', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "    print(X_test.shape)\n",
    "    print(X_test)\n",
    "with open('../DataSet15/y_test_allshuffled.npy', 'rb') as f:\n",
    "    Y_test = pickle.load(f)\n",
    "    print(Y_test.shape)\n",
    "    print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model created after picking best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_final(X_train, Y_train, checkpoint_path, Conv_filter, Conv_layers, Dense_layers, Dense_filter):\n",
    "    ## Callbacks\n",
    "    '''\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('loss')>0.99):\n",
    "                print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "    callback = myCallback()\n",
    "    '''\n",
    "    \n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True, restore_best_weights=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False)\n",
    "\n",
    "    \n",
    "    ## Model\n",
    "    model = tf.keras.models.Sequential()             \n",
    "    model.add(tf.keras.layers.Conv1D(Conv_filter, (4), activation='relu', input_shape=(1000, 4)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "    model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    for l in range(Conv_layers):\n",
    "        model.add(tf.keras.layers.Conv1D(Conv_filter, (3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(4))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "    model.add(tf.keras.layers.LSTM(64))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for l in range(Dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(Dense_filter, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))      \n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    ## Fitting Model\n",
    "    #history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n",
    "    #                    callbacks=[earlystop, cp_callback], validation_split=0.3, \n",
    "    #                    validation_batch_size=val_batch_size, shuffle=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the final model on the Testing Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 - 10s - loss: 0.4552 - accuracy: 0.7934\n",
      "Restored model, accuracy: 79.34%\n",
      "WARNING:tensorflow:From <ipython-input-32-1964d23b7a23>:9: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "0\n",
      "16382\n",
      "[[6061 1656]\n",
      " [1728 6937]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "# Re-evaluate the model\n",
    "loss, acc = model.evaluate(X_test,  Y_test, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
    "\n",
    "predicted = (model.predict_classes(X_test))\n",
    "\n",
    "expected = []\n",
    "\n",
    "for label in Y_test:\n",
    "    expected.append(label[1])\n",
    "print(expected[0])\n",
    "print(len(expected))\n",
    "\n",
    "results = confusion_matrix(expected, predicted)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 997, 128)          2176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 249, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 249, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 247, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 61, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 142,466\n",
      "Trainable params: 142,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final Confusion Matrix\n",
    "sns.heatmap(results/np.sum(results), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "011_E003_CNN_DataSet13_2categories_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
